{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
      "<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
      "<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n"
     ]
    }
   ],
   "source": [
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "import graphviz \n",
    "# from relation_algorithms.relation_extraction_functions import LLM_Relation_Extractor\n",
    "from src.extractor import relationExtractor\n",
    "import os\n",
    "import pandas as pd\n",
    "from src.utils import normalize_text\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "dsa_2214_link = getenv('dsa_2214')\n",
    "dsa_6114_link = getenv('dsa_6114')\n",
    "hadoop_link = getenv('hadoop')\n",
    "token = getenv('OPENAI_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Structures-&gt;Basics-&gt;Algorithm Analysis</td>\n",
       "      <td>Apply time complexity analysis guideline to an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Structures-&gt;Basics-&gt;Algorithm Analysis-&gt;O...</td>\n",
       "      <td>Demonstrate an understanding of big O notation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algorithms-&gt;Non-recursive Algorthims</td>\n",
       "      <td>Demonstrate an understanding of non-recursive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Algorithms-&gt;Non-recursive Algorithms-&gt;Search</td>\n",
       "      <td>Apply the Comparable interface for object comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Algorithms-&gt;Non-recursive Algorithms-&gt;Search-&gt;...</td>\n",
       "      <td>Demonstrate an understanding of linear search;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Algorithms-&gt;Non-recursive Algorithms-&gt;Search-&gt;...</td>\n",
       "      <td>Demonstrate an understanding of binary search;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Algorithms-&gt;Non-recursive Algorithms-&gt;Sort</td>\n",
       "      <td>Demonstrate an understanding of sorting;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Algorithms-&gt;Non-recursive Algorithms-&gt;Sort-&gt;In...</td>\n",
       "      <td>Demonstrate an understanding of insertion sort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Algorithms-&gt;Non-recursive Algorithms-&gt;Sort-&gt;Se...</td>\n",
       "      <td>Demonstrate an understanding of selection sort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Algorithms-&gt;Non-recursive Algorithms-&gt;Sort-&gt;Bu...</td>\n",
       "      <td>Demonstrate an understanding of bubble sort;An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Algorithms-&gt;Recursive Algorithms-&gt;Recursive Al...</td>\n",
       "      <td>Demonstrate an understanding of recursion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Algorithms-&gt;Recursive Algorithms-&gt;Recursive Bi...</td>\n",
       "      <td>Demonstrate an understanding of recursive bina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Algorithms-&gt;Recursive Algorithms-&gt;Recursive So...</td>\n",
       "      <td>Demonstrate an understanding of recursive merg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Algorithms-&gt;Recursive Algorithms-&gt;Recursive So...</td>\n",
       "      <td>Demonstrate an understanding of recursive quic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              concept  \\\n",
       "0         Data Structures->Basics->Algorithm Analysis   \n",
       "1   Data Structures->Basics->Algorithm Analysis->O...   \n",
       "2                Algorithms->Non-recursive Algorthims   \n",
       "3        Algorithms->Non-recursive Algorithms->Search   \n",
       "4   Algorithms->Non-recursive Algorithms->Search->...   \n",
       "5   Algorithms->Non-recursive Algorithms->Search->...   \n",
       "6          Algorithms->Non-recursive Algorithms->Sort   \n",
       "7   Algorithms->Non-recursive Algorithms->Sort->In...   \n",
       "8   Algorithms->Non-recursive Algorithms->Sort->Se...   \n",
       "9   Algorithms->Non-recursive Algorithms->Sort->Bu...   \n",
       "10  Algorithms->Recursive Algorithms->Recursive Al...   \n",
       "11  Algorithms->Recursive Algorithms->Recursive Bi...   \n",
       "12  Algorithms->Recursive Algorithms->Recursive So...   \n",
       "13  Algorithms->Recursive Algorithms->Recursive So...   \n",
       "\n",
       "                                              outcome  \n",
       "0   Apply time complexity analysis guideline to an...  \n",
       "1   Demonstrate an understanding of big O notation...  \n",
       "2   Demonstrate an understanding of non-recursive ...  \n",
       "3   Apply the Comparable interface for object comp...  \n",
       "4   Demonstrate an understanding of linear search;...  \n",
       "5   Demonstrate an understanding of binary search;...  \n",
       "6            Demonstrate an understanding of sorting;  \n",
       "7   Demonstrate an understanding of insertion sort...  \n",
       "8   Demonstrate an understanding of selection sort...  \n",
       "9   Demonstrate an understanding of bubble sort;An...  \n",
       "10          Demonstrate an understanding of recursion  \n",
       "11  Demonstrate an understanding of recursive bina...  \n",
       "12  Demonstrate an understanding of recursive merg...  \n",
       "13  Demonstrate an understanding of recursive quic...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/sorting.csv')\n",
    "data.columns = ['concept', 'outcome']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_data = data['concept'].tolist()\n",
    "actual_concepts = []\n",
    "for string in concept_data:\n",
    "    words = string.split('->')\n",
    "    for word in words:\n",
    "        if word not in actual_concepts:\n",
    "            actual_concepts.append(word)\n",
    "\n",
    "\n",
    "outcome_data = data['outcome'].tolist()\n",
    "actual_outcomes = []\n",
    "for string in outcome_data:\n",
    "    words = string.split(';')\n",
    "    for s in words:\n",
    "        if s not in actual_outcomes:\n",
    "            actual_outcomes.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsa_2214_chapters = [\n",
    "    'Data Structures and Algorithms',\n",
    "    'Mathematical Preliminaries',\n",
    "    'Algorithm Analysis',\n",
    "    'Lists, Stacks, and Queues',\n",
    "    'Binary Trees',\n",
    "    'Non-Binary Trees',\n",
    "    'Internal Sorting',\n",
    "    'File Processing and External Sorting',\n",
    "    'Searching',\n",
    "    'Indexing',\n",
    "    'Graphs',\n",
    "    'Lists and Arrays Revisited',\n",
    "    'Advanced Tree Structures',\n",
    "    'Analysis Techniques',\n",
    "    'Lower Bounds',\n",
    "    'Patterns of Algorithms',\n",
    "    'Limits to Computation',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_concepts = [' '.join(actual_concepts)] * 4\n",
    "actual_outcomes = [' '.join(actual_outcomes)] * 4\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from src.transformerEmbeddings import TransformerEmbeddings\n",
    "\n",
    "eval_llm = LangchainLLMWrapper(langchain_llm = ChatOpenAI(model = 'gpt-4o-mini'))\n",
    "eval_embeddings = LangchainEmbeddingsWrapper(embeddings = TransformerEmbeddings(model = 'all-MiniLM-L12-v2'))\n",
    "\n",
    "from ragas.metrics import ResponseRelevancy, LLMContextRecall, SemanticSimilarity, Faithfulness, NoiseSensitivity, LLMContextPrecisionWithReference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [ResponseRelevancy(), LLMContextPrecisionWithReference(), LLMContextRecall(), SemanticSimilarity(), Faithfulness(), NoiseSensitivity()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Testing new retrieval class design... (PLEASE WORK)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.extractor import relationExtractor\n",
    "extractor = relationExtractor(dsa_2214_link, \n",
    "                            token, \n",
    "                            dsa_2214_chapters[6:10], \n",
    "                            getenv('connection_string'),\n",
    "                            3000,\n",
    "                            100,\n",
    "                            'DocumentEmbeddings',\n",
    "                            '2214_embeddings',\n",
    "                            st_model = 'all-MiniLM-L6-v2',\n",
    "                            reset = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zanehutchens/uncc/research/rag/src/extractor.py:303: DeprecationWarning: invalid escape sequence '\\s'\n",
      "  relation = re.sub(re.compile('^[a-zA-Z\\s\\.,!?]'), '', relation)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 240.00 MiB (GPU 0; 3.71 GiB total capacity; 2.26 GiB already allocated; 148.75 MiB free; 2.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m retrieved, concepts \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midentify_concepts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/uncc/research/rag/src/extractor.py:382\u001b[0m, in \u001b[0;36mrelationExtractor.identify_concepts\u001b[0;34m(self, num_concepts)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/src/extractor.py?line=379'>380</a>\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchapters:\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/src/extractor.py?line=380'>381</a>\u001b[0m     prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mIdentify \u001b[39m\u001b[39m{\u001b[39;00mnum_concepts\u001b[39m}\u001b[39;00m\u001b[39m learning concepts from chapter \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> <a href='file:///home/zanehutchens/uncc/research/rag/src/extractor.py?line=381'>382</a>\u001b[0m     relevant_docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieval_pipeline(prompt, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlink)\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/src/extractor.py?line=383'>384</a>\u001b[0m     \u001b[39m# # single shot prompt \u001b[39;00m\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/src/extractor.py?line=384'>385</a>\u001b[0m     single_prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'''\u001b[39m\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/src/extractor.py?line=385'>386</a>\u001b[0m \u001b[39m                     Identify the \u001b[39m\u001b[39m{\u001b[39;00mnum_concepts\u001b[39m}\u001b[39;00m\u001b[39m most important learning concepts for chapter: \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/src/extractor.py?line=386'>387</a>\u001b[0m \u001b[39m                     The relevant context can be found here: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(t[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mpage_content\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mt\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39mrelevant_docs)\u001b[39m}\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/src/extractor.py?line=387'>388</a>\u001b[0m \u001b[39m                     \u001b[39m\u001b[39m'''\u001b[39m\n",
      "File \u001b[0;32m~/uncc/research/rag/src/extractor.py:124\u001b[0m, in \u001b[0;36mrelationExtractor.retrieval_pipeline\u001b[0;34m(self, query, context, num_queries, top_n)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/src/extractor.py?line=120'>121</a>\u001b[0m         retrieved_docs\u001b[39m.\u001b[39mappend((q, doc))\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/src/extractor.py?line=122'>123</a>\u001b[0m \u001b[39m# rank and return top_n docs\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/zanehutchens/uncc/research/rag/src/extractor.py?line=123'>124</a>\u001b[0m \u001b[39mreturn\u001b[39;00m rank_docs(retrieved_docs, top_n \u001b[39m=\u001b[39;49m top_n)\n",
      "File \u001b[0;32m~/uncc/research/rag/src/utils.py:29\u001b[0m, in \u001b[0;36mrank_docs\u001b[0;34m(queries_and_docs, top_n)\u001b[0m\n\u001b[1;32m     <a href='file:///home/zanehutchens/uncc/research/rag/src/utils.py?line=17'>18</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/zanehutchens/uncc/research/rag/src/utils.py?line=18'>19</a>\u001b[0m \u001b[39mRanks queries and documents using CrossEncoder\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/zanehutchens/uncc/research/rag/src/utils.py?line=19'>20</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/zanehutchens/uncc/research/rag/src/utils.py?line=25'>26</a>\u001b[0m \u001b[39m    list: list of top_n documents\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/zanehutchens/uncc/research/rag/src/utils.py?line=26'>27</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/zanehutchens/uncc/research/rag/src/utils.py?line=27'>28</a>\u001b[0m model \u001b[39m=\u001b[39m CrossEncoder(model_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmixedbread-ai/mxbai-rerank-base-v1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='file:///home/zanehutchens/uncc/research/rag/src/utils.py?line=28'>29</a>\u001b[0m scores \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict([(t[\u001b[39m0\u001b[39;49m], t[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mpage_content) \u001b[39mfor\u001b[39;49;00m t \u001b[39min\u001b[39;49;00m queries_and_docs])\n\u001b[1;32m     <a href='file:///home/zanehutchens/uncc/research/rag/src/utils.py?line=30'>31</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msorted\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(queries_and_docs, scores)), key \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m], reverse \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)[:top_n]\n",
      "File \u001b[0;32m~/uncc/research/rag/env/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:444\u001b[0m, in \u001b[0;36mCrossEncoder.predict\u001b[0;34m(self, sentences, batch_size, show_progress_bar, num_workers, activation_fct, apply_softmax, convert_to_numpy, convert_to_tensor)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py?line=441'>442</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py?line=442'>443</a>\u001b[0m     \u001b[39mfor\u001b[39;00m features \u001b[39min\u001b[39;00m iterator:\n\u001b[0;32m--> <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py?line=443'>444</a>\u001b[0m         model_predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfeatures, return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py?line=444'>445</a>\u001b[0m         logits \u001b[39m=\u001b[39m activation_fct(model_predictions\u001b[39m.\u001b[39mlogits)\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py?line=446'>447</a>\u001b[0m         \u001b[39mif\u001b[39;00m apply_softmax \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(logits[\u001b[39m0\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1176\u001b[0m, in \u001b[0;36mDebertaV2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=1167'>1168</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=1168'>1169</a>\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=1169'>1170</a>\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=1170'>1171</a>\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=1171'>1172</a>\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=1172'>1173</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=1173'>1174</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=1175'>1176</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeberta(\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=1176'>1177</a>\u001b[0m     input_ids,\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=1177'>1178</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=1178'>1179</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=1179'>1180</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=1180'>1181</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=1181'>1182</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=1182'>1183</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=1183'>1184</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=1184'>1185</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=1186'>1187</a>\u001b[0m encoder_layer \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=1187'>1188</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(encoder_layer)\n",
      "File \u001b[0;32m~/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:871\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=860'>861</a>\u001b[0m     token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=862'>863</a>\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=863'>864</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=864'>865</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=867'>868</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=868'>869</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=870'>871</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=871'>872</a>\u001b[0m     embedding_output,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=872'>873</a>\u001b[0m     attention_mask,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=873'>874</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=874'>875</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=875'>876</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=876'>877</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=877'>878</a>\u001b[0m encoded_layers \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=879'>880</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz_steps \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:675\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=664'>665</a>\u001b[0m     output_states, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=665'>666</a>\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=666'>667</a>\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=671'>672</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=672'>673</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=673'>674</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=674'>675</a>\u001b[0m     output_states, attn_weights \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=675'>676</a>\u001b[0m         next_kv,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=676'>677</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=677'>678</a>\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=678'>679</a>\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=679'>680</a>\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=680'>681</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=681'>682</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=683'>684</a>\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=684'>685</a>\u001b[0m     all_attentions \u001b[39m=\u001b[39m all_attentions \u001b[39m+\u001b[39m (attn_weights,)\n",
      "File \u001b[0;32m~/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:443\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=433'>434</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=434'>435</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=435'>436</a>\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=440'>441</a>\u001b[0m     output_attentions: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=441'>442</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, Optional[torch\u001b[39m.\u001b[39mTensor]]:\n\u001b[0;32m--> <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=442'>443</a>\u001b[0m     attention_output, att_matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=443'>444</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=444'>445</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=445'>446</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=446'>447</a>\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=447'>448</a>\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=448'>449</a>\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=449'>450</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=450'>451</a>\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=451'>452</a>\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n",
      "File \u001b[0;32m~/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:376\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=366'>367</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=367'>368</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=368'>369</a>\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=373'>374</a>\u001b[0m     rel_embeddings\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=374'>375</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, Optional[torch\u001b[39m.\u001b[39mTensor]]:\n\u001b[0;32m--> <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=375'>376</a>\u001b[0m     self_output, att_matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=376'>377</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=377'>378</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=378'>379</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=379'>380</a>\u001b[0m         query_states\u001b[39m=\u001b[39;49mquery_states,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=380'>381</a>\u001b[0m         relative_pos\u001b[39m=\u001b[39;49mrelative_pos,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=381'>382</a>\u001b[0m         rel_embeddings\u001b[39m=\u001b[39;49mrel_embeddings,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=382'>383</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=383'>384</a>\u001b[0m     \u001b[39mif\u001b[39;00m query_states \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=384'>385</a>\u001b[0m         query_states \u001b[39m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:255\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=252'>253</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelative_attention:\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=253'>254</a>\u001b[0m     rel_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_dropout(rel_embeddings)\n\u001b[0;32m--> <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=254'>255</a>\u001b[0m     rel_att \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisentangled_attention_bias(\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=255'>256</a>\u001b[0m         query_layer, key_layer, relative_pos, rel_embeddings, scale_factor\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=256'>257</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=258'>259</a>\u001b[0m \u001b[39mif\u001b[39;00m rel_att \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=259'>260</a>\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m rel_att\n",
      "File \u001b[0;32m~/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:349\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.disentangled_attention_bias\u001b[0;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=346'>347</a>\u001b[0m     p2c_pos \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mclamp(\u001b[39m-\u001b[39mr_pos \u001b[39m+\u001b[39m att_span, \u001b[39m0\u001b[39m, att_span \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=347'>348</a>\u001b[0m     p2c_att \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbmm(key_layer, pos_query_layer\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m))\n\u001b[0;32m--> <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=348'>349</a>\u001b[0m     p2c_att \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mgather(\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=349'>350</a>\u001b[0m         p2c_att,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=350'>351</a>\u001b[0m         dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=351'>352</a>\u001b[0m         index\u001b[39m=\u001b[39;49mp2c_pos\u001b[39m.\u001b[39;49msqueeze(\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mexpand([query_layer\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m), key_layer\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m), key_layer\u001b[39m.\u001b[39;49msize(\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m)]),\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=352'>353</a>\u001b[0m     )\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=353'>354</a>\u001b[0m     score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m p2c_att \u001b[39m/\u001b[39m scale\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mp2c_att\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    <a href='file:///home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py?line=355'>356</a>\u001b[0m \u001b[39mreturn\u001b[39;00m score\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 240.00 MiB (GPU 0; 3.71 GiB total capacity; 2.26 GiB already allocated; 148.75 MiB free; 2.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "retrieved, concepts = extractor.identify_concepts(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concepts evaluation without text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = extractor.evaluate('concepts', 5, concepts, actual_concepts, metrics = metrics, data = retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concepts evaluation with text normalization on retrieved contexts and concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_concepts = [[normalize_text(' '.join(t))] for t in concepts]\n",
    "normalized_truths = [normalize_text(t) for t in actual_concepts]\n",
    "\n",
    "normalized_retrieved = {}\n",
    "for k in retrieved.keys():\n",
    "    normalized_retrieved[k] = normalize_text(retrieved[k])\n",
    "\n",
    "# for i in range(5):\n",
    "normalized_samples = extractor.evaluate('concepts', 5, normalized_concepts, normalized_truths, data = normalized_retrieved, metrics = metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outcome evaluation using concepts without text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes, outcome_contexts = extractor.identify_outcomes(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = extractor.evaluate('outcomes', 5, outcomes, actual_outcomes, metrics = metrics, data = retrieved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outcome evaluation with text normalization on outcomes and retrieved contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_outcomes = [[normalize_text(' '.join(t))] for t in outcomes]\n",
    "normalized_truths = [normalize_text(t) for t in actual_outcomes]\n",
    "\n",
    "normalized_retrieved = {}\n",
    "for k in retrieved.keys():\n",
    "    normalized_retrieved[k] = normalize_text(outcome_contexts[k])\n",
    "\n",
    "samples = extractor.evaluate('outcomes', 5, normalized_outcomes, normalized_truths, data = normalized_retrieved, metrics = metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chp4_concepts = []\n",
    "with open('data/chp4.txt', 'r') as f:\n",
    "    for l in f.readlines():\n",
    "        concepts = l.strip().split('->')\n",
    "        for c in concepts:\n",
    "            if c not in chp4_concepts: chp4_concepts.append(c)\n",
    "\n",
    "print(chp4_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.build_terminology([[word] for word in chp4_concepts])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "56211c6593777bedeb9a19153ebc7701344247d055e998aec252a1f471490a08"
  },
  "kernelspec": {
   "display_name": "Python 3.10.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
