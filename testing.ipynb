{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ragas.metrics as m\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "# from ragas.llms import LangchainLLMWrapper\n",
    "# from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "load_dotenv()\n",
    "link = os.getenv('dsa_2214')\n",
    "token = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = token\n",
    "\n",
    "chapters = [\n",
    "    'Data Structures and Algorithms',\n",
    "    'Mathematical Preliminaries',\n",
    "    'Algorithm Analysis',\n",
    "    'Lists, Stacks, and Queues',\n",
    "    'Binary Trees',\n",
    "    'Non-Binary Trees',\n",
    "    'Internal Sorting',\n",
    "    'File Processing and External Sorting',\n",
    "    'Searching',\n",
    "    'Indexing',\n",
    "    'Graphs',\n",
    "    'Lists and Arrays Revisited',\n",
    "    'Advanced Tree Structures',\n",
    "    'Analysis Techniques',\n",
    "    'Lower Bounds',\n",
    "    'Patterns of Algorithms',\n",
    "    'Limits to Computation',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concept</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Structures-&gt;Basics-&gt;Algorithm Analysis</td>\n",
       "      <td>Apply time complexity analysis guideline to an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Structures-&gt;Basics-&gt;Algorithm Analysis-&gt;O...</td>\n",
       "      <td>Demonstrate an understanding of big O notation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algorithms-&gt;Non-recursive Algorthims</td>\n",
       "      <td>Demonstrate an understanding of non-recursive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Algorithms-&gt;Non-recursive Algorithms-&gt;Search</td>\n",
       "      <td>Apply the Comparable interface for object comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Algorithms-&gt;Non-recursive Algorithms-&gt;Search-&gt;...</td>\n",
       "      <td>Demonstrate an understanding of linear search;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Algorithms-&gt;Non-recursive Algorithms-&gt;Search-&gt;...</td>\n",
       "      <td>Demonstrate an understanding of binary search;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Algorithms-&gt;Non-recursive Algorithms-&gt;Sort</td>\n",
       "      <td>Demonstrate an understanding of sorting;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Algorithms-&gt;Non-recursive Algorithms-&gt;Sort-&gt;In...</td>\n",
       "      <td>Demonstrate an understanding of insertion sort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Algorithms-&gt;Non-recursive Algorithms-&gt;Sort-&gt;Se...</td>\n",
       "      <td>Demonstrate an understanding of selection sort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Algorithms-&gt;Non-recursive Algorithms-&gt;Sort-&gt;Bu...</td>\n",
       "      <td>Demonstrate an understanding of bubble sort;An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Algorithms-&gt;Recursive Algorithms-&gt;Recursive Al...</td>\n",
       "      <td>Demonstrate an understanding of recursion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Algorithms-&gt;Recursive Algorithms-&gt;Recursive Bi...</td>\n",
       "      <td>Demonstrate an understanding of recursive bina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Algorithms-&gt;Recursive Algorithms-&gt;Recursive So...</td>\n",
       "      <td>Demonstrate an understanding of recursive merg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Algorithms-&gt;Recursive Algorithms-&gt;Recursive So...</td>\n",
       "      <td>Demonstrate an understanding of recursive quic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              concept  \\\n",
       "0         Data Structures->Basics->Algorithm Analysis   \n",
       "1   Data Structures->Basics->Algorithm Analysis->O...   \n",
       "2                Algorithms->Non-recursive Algorthims   \n",
       "3        Algorithms->Non-recursive Algorithms->Search   \n",
       "4   Algorithms->Non-recursive Algorithms->Search->...   \n",
       "5   Algorithms->Non-recursive Algorithms->Search->...   \n",
       "6          Algorithms->Non-recursive Algorithms->Sort   \n",
       "7   Algorithms->Non-recursive Algorithms->Sort->In...   \n",
       "8   Algorithms->Non-recursive Algorithms->Sort->Se...   \n",
       "9   Algorithms->Non-recursive Algorithms->Sort->Bu...   \n",
       "10  Algorithms->Recursive Algorithms->Recursive Al...   \n",
       "11  Algorithms->Recursive Algorithms->Recursive Bi...   \n",
       "12  Algorithms->Recursive Algorithms->Recursive So...   \n",
       "13  Algorithms->Recursive Algorithms->Recursive So...   \n",
       "\n",
       "                                              outcome  \n",
       "0   Apply time complexity analysis guideline to an...  \n",
       "1   Demonstrate an understanding of big O notation...  \n",
       "2   Demonstrate an understanding of non-recursive ...  \n",
       "3   Apply the Comparable interface for object comp...  \n",
       "4   Demonstrate an understanding of linear search;...  \n",
       "5   Demonstrate an understanding of binary search;...  \n",
       "6            Demonstrate an understanding of sorting;  \n",
       "7   Demonstrate an understanding of insertion sort...  \n",
       "8   Demonstrate an understanding of selection sort...  \n",
       "9   Demonstrate an understanding of bubble sort;An...  \n",
       "10          Demonstrate an understanding of recursion  \n",
       "11  Demonstrate an understanding of recursive bina...  \n",
       "12  Demonstrate an understanding of recursive merg...  \n",
       "13  Demonstrate an understanding of recursive quic...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/sorting.csv')\n",
    "data.columns = ['concept', 'outcome']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_data = data['concept'].tolist()\n",
    "actual_concepts = []\n",
    "for string in concept_data:\n",
    "    words = string.split('->')\n",
    "    for word in words:\n",
    "        if word not in actual_concepts:\n",
    "            actual_concepts.append(word)\n",
    "\n",
    "\n",
    "outcome_data = data['outcome'].tolist()\n",
    "actual_outcomes = []\n",
    "for string in outcome_data:\n",
    "    words = string.split(';')\n",
    "    for s in words:\n",
    "        if s not in actual_outcomes:\n",
    "            actual_outcomes.append(s)\n",
    "\n",
    "actual_concepts = [' '.join(actual_concepts)] * 4\n",
    "actual_outcomes = [' '.join(actual_outcomes)] * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
      "<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
      "<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n",
      "/home/zanehutchens/uncc/research/rag/env/lib/python3.10/site-packages/pydantic/_internal/_config.py:291: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
      "  warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from src.extractor import relationExtractor\n",
    "extractor = relationExtractor(link, token, chapters[6:10], os.getenv('connection_string'), 3000, 100, 'DocumentEmbeddings', '2214_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Testing deepeval...</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts, retrieved = extractor.identify_concepts(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Precision Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Precision Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Recall Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Recall Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 4 test case(s) in parallel: |██████████|100% (4/4) [Time Taken: 00:15,  3.98s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is fully relevant and directly addresses the task of identifying key learning concepts for the specified chapter. Great job staying focused and on point!, error: None)\n",
      "  - ❌ Contextual Precision (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the first node in the retrieval context, which discusses 'Mergesort and its implementation', is not relevant to the input, which seeks 'important learning concepts for chapter File Processing and External Sorting'. Since there are no relevant nodes ranked higher than this irrelevant node, the score remains at 0.00., error: None)\n",
      "  - ❌ Contextual Recall (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the entire expected output sentence does not match or align with any information or structure found in the nodes in the retrieval context, resulting in no supportive connections., error: None)\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions, indicating perfect alignment between the actual output and the retrieval context. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Identify the 5 most important learning concepts for chapter File Processing and External Sorting. The relevant context can be found here: 234 Chap. 7 Internal Sorting 36 20 17 13 28 14 23 15 28 23 15 14 36 20 17 13 20 36 13 17 14 28 15 23 13 14 15 17 20 23 28 36 Figure 7.8 An illustration of Mergesort. The ﬁrst row shows eight numbers that are to be sorted. Mergesort will recursively subdivide the list into sublists of one element each, then recombine the sublists. The second row shows the four sublists of size 2 created by the ﬁrst merging pass. The third row shows the two sublists of size 4 created by the next merging pass on the sublists of row 2. The last row shows the ﬁnal sorted list created by merging the two sublists of row 3. Mergesort is one of the simplest sorting algorithms conceptually, and has good performance both in the asymptotic sense and in empirical running time. Surpris- ingly, even though it is based on a simple concept, it is relatively difﬁcult to im- plement in practice. Figure 7.8 illustrates Mergesort. A pseudocode sketch of Mergesort is as follows: List mergesort(List inlist) { if (inlist.length() <= 1) return inlist;; List L1 = half of the items from inlist; List L2 = other half of the items from inlist; return merge(mergesort(L1), mergesort(L2)); } Before discussing how to implement Mergesort, we will ﬁrst examine the merge function. Merging two sorted sublists is quite simple. Function merge examines the ﬁrst element of each sublist and picks the smaller value as the smallest element overall. This smaller value is removed from its sublist and placed into the output list. Merging continues in this way, comparing the front elements of the sublists and continually appending the smaller to the output list until no more input elements remain. Implementing Mergesort presents a number of technical difﬁculties. The ﬁrst decision is how to represent the lists. Mergesort lends itself well to sorting a singly linked list because merging does not require random access to the list elements. Thus, Mergesort is the method of choice when the input is in the form of a linked list. Implementing merge for linked lists is straightforward, because we need only remove items from the front of the input lists and append items to the output list. Breaking the input list into two equal halves presents some difﬁculty.These assumptions can be  284 Chap. 8 File Processing and External Sorting relaxed for special-purpose sorting applications, but ignoring such complications makes the principles clearer. As explained in Section 8.2, a sector is the basic unit of I/O. In other words, all disk reads and writes are for one or more complete sectors. Sector sizes are typically a power of two, in the range 512 to 16K bytes, depending on the operating system and the size and speed of the disk drive. The block size used for external sorting algorithms should be equal to or a multiple of the sector size. Under this model, a sorting algorithm reads a block of data into a buffer in main memory, performs some processing on it, and at some future time writes it back to disk. From Section 8.1 we see that reading or writing a block from disk takes on the order of one million times longer than a memory access. Based on this fact, we can reasonably expect that the records contained in a single block can be sorted by an internal sorting algorithm such as Quicksort in less time than is required to read or write the block. Under good conditions, reading from a ﬁle in sequential order is more efﬁcient than reading blocks in random order. Given the signiﬁcant impact of seek time on disk access, it might seem obvious that sequential processing is faster. However, it is important to understand precisely under what circumstances sequential ﬁle processing is actually faster than random access, because it affects our approach to designing an external sorting algorithm. Efﬁcient sequential access relies on seek time being kept to a minimum. The ﬁrst requirement is that the blocks making up a ﬁle are in fact stored on disk in sequential order and close together, preferably ﬁlling a small number of contiguous tracks. At the very least, the number of extents making up the ﬁle should be small. Users typically do not have much control over the layout of their ﬁle on disk, but writing a ﬁle all at once in sequential order to a disk drive with a high percentage of free space increases the likelihood of such an arrangement. The second requirement is that the disk drive’s I/O head remain positioned over the ﬁle throughout sequential processing. This will not happen if there is competition of any kind for the I/O head. For example, on a multi-user time-shared computer the sorting process might compete for the I/O head with the processes of other users. Even when the sorting process has sole control of the I/O head, it is still likely that sequential processing will not be efﬁcient. Imagine the situation where all processing is done on a single disk drive, with the typical arrangement of a single bank of read/write heads that move together over a stack of platters. If the sorting process involves reading from an input ﬁle, alternated with writing to an output ﬁle, then the I/O head will continuously seek between the input ﬁle and the output ﬁle. Similarly, if two input ﬁles are being processed simultaneously (such as during a merge process), then the I/O head will continuously seek between these two ﬁles. Sec. 8.5 External Sorting 285 The moral is that, with a single disk drive, there often is no such thing as efﬁ- cient sequential processing of a data ﬁle. Thus, a sorting algorithm might be more efﬁcient if it performs a smaller number of non-sequential disk operations rather than a larger number of logically sequential disk operations that require a large number of seeks in practice. As mentioned previously, the record size might be quite large compared to the size of the key.Trees are typically used to or- ganize large databases that must support record insertion, deletion, and key range searches. Section 10.2 brieﬂy describes ISAM, a tentative step toward solving the problem of storing a large database that must support insertion and deletion of records. Its shortcomings help to illustrate the value of tree indexing techniques.274 Chap. 8 File Processing and External Sorting This example illustrates why it is important to keep disk ﬁles from be- coming fragmented, and why so-called “disk defragmenters” can speed up ﬁle processing time. File fragmentation happens most commonly when the disk is nearly full and the ﬁle manager must search for free space whenever a ﬁle is created or changed. 8.3 Buﬀers and Buﬀer Pools Given the speciﬁcations of the disk drive from Example 8.1, we ﬁnd that it takes about 9.5+11.1×1.5 = 26.2 ms to read one track of data on average. It takes about 9.5+11.1/2+(1/256)×11.1 = 15.1 ms on average to read a single sector of data. This is a good savings (slightly over half the time), but less than 1% of the data on the track are read. If we want to read only a single byte, it would save us effectively no time over that required to read an entire sector. For this reason, nearly all disk drives automatically read or write an entire sector’s worth of information whenever the disk is accessed, even when only one byte of information is requested. Once a sector is read, its information is stored in main memory. This is known as buffering or caching the information. If the next disk request is to that same sector, then it is not necessary to read from disk again because the information is already stored in main memory. Buffering is an example of one method for minimizing disk accesses mentioned at the beginning of the chapter: Bring off additional information from disk to satisfy future requests. If information from ﬁles were accessed at random, then the chance that two consecutive disk requests are to the same sector would be low. However, in practice most disk requests are close to the location (in the logical ﬁle at least) of the previous request. This means that the probability of the next request “hitting the cache” is much higher than chance would indicate. This principle explains one reason why average access times for new disk drives are lower than in the past. Not only is the hardware faster, but information is also now stored using better algorithms and larger caches that minimize the number of times information needs to be fetched from disk. This same concept is also used to store parts of programs in faster memory within the CPU, using the CPU cache that is prevalent in modern microprocessors. Sector-level buffering is normally provided by the operating system and is of- ten built directly into the disk drive controller hardware. Most operating systems maintain at least two buffers, one for input and one for output. Consider what would happen if there were only one buffer during a byte-by-byte copy operation. The sector containing the ﬁrst byte would be read into the I/O buffer. The output operation would need to destroy the contents of the single I/O buffer to write this byte. Then the buffer would need to be ﬁlled again from disk for the second byte,  Sec. 8.3 Buﬀers and Buﬀer Pools 275 only to be destroyed during output. The simple solution to this problem is to keep one buffer for input, and a second for output. Most disk drive controllers operate independently from the CPU once an I/O request is received. This is useful because the CPU can typically execute millions of instructions during the time required for a single I/O operation..\n",
      "  - actual output: Here are the five most important learning concepts for the chapter on File Processing and External Sorting: 1. **Understanding External Sorting**: External sorting is crucial for handling large datasets that do not fit into main memory. It involves reading data from disk, processing it in blocks, and writing the sorted data back to disk. Key algorithms, such as Mergesort, are adapted for external sorting to minimize disk I/O operations, which are significantly slower than memory accesses. 2. **Mergesort Algorithm**: Mergesort is a fundamental sorting algorithm that works by recursively dividing a list into smaller sublists, sorting those sublists, and then merging them back together. Understanding its implementation, especially the merge function, is essential for grasping how sorting can be efficiently performed on large datasets. 3. **Disk I/O and Buffering**: The chapter emphasizes the importance of disk I/O operations and the concept of buffering. Since reading and writing data from/to disk is much slower than accessing data in memory, effective buffering strategies are necessary to minimize the number of disk accesses. This includes understanding how to manage buffers for input and output operations. 4. **Sequential vs. Random Access**: The efficiency of file processing is heavily influenced by the access patterns of data. Sequential access is generally faster than random access due to reduced seek times. Understanding the conditions under which sequential processing is more efficient is critical for designing effective external sorting algorithms. 5. **File Fragmentation and Performance**: File fragmentation can significantly impact the performance of file processing. The chapter discusses how fragmented files can lead to increased seek times and slower access speeds. Techniques such as disk defragmentation are important for maintaining optimal file performance, especially in the context of external sorting. These concepts provide a foundational understanding of how file processing and external sorting work, particularly in the context of managing large datasets efficiently.\n",
      "  - expected output: Data Structures Basics Algorithm Analysis O Notation Algorithms Non-recursive Algorthims Non-recursive Algorithms Search Linear Search Binary Search Sort Insertion Sort Selection Sort Bubble Sort Recursive Algorithms Recursive Binary Search Recursive Sort Merge Sort Quick Sort\n",
      "  - context: None\n",
      "  - retrieval context: ['234 Chap. 7 Internal Sorting 36 20 17 13 28 14 23 15 28 23 15 14 36 20 17 13 20 36 13 17 14 28 15 23 13 14 15 17 20 23 28 36 Figure 7.8 An illustration of Mergesort. The ﬁrst row shows eight numbers that are to be sorted. Mergesort will recursively subdivide the list into sublists of one element each, then recombine the sublists. The second row shows the four sublists of size 2 created by the ﬁrst merging pass. The third row shows the two sublists of size 4 created by the next merging pass on the sublists of row 2. The last row shows the ﬁnal sorted list created by merging the two sublists of row 3. Mergesort is one of the simplest sorting algorithms conceptually, and has good performance both in the asymptotic sense and in empirical running time. Surpris- ingly, even though it is based on a simple concept, it is relatively difﬁcult to im- plement in practice. Figure 7.8 illustrates Mergesort. A pseudocode sketch of Mergesort is as follows: List mergesort(List inlist) { if (inlist.length() <= 1) return inlist;; List L1 = half of the items from inlist; List L2 = other half of the items from inlist; return merge(mergesort(L1), mergesort(L2)); } Before discussing how to implement Mergesort, we will ﬁrst examine the merge function. Merging two sorted sublists is quite simple. Function merge examines the ﬁrst element of each sublist and picks the smaller value as the smallest element overall. This smaller value is removed from its sublist and placed into the output list. Merging continues in this way, comparing the front elements of the sublists and continually appending the smaller to the output list until no more input elements remain. Implementing Mergesort presents a number of technical difﬁculties. The ﬁrst decision is how to represent the lists. Mergesort lends itself well to sorting a singly linked list because merging does not require random access to the list elements. Thus, Mergesort is the method of choice when the input is in the form of a linked list. Implementing merge for linked lists is straightforward, because we need only remove items from the front of the input lists and append items to the output list. Breaking the input list into two equal halves presents some difﬁculty.These assumptions can be  284 Chap. 8 File Processing and External Sorting relaxed for special-purpose sorting applications, but ignoring such complications makes the principles clearer. As explained in Section 8.2, a sector is the basic unit of I/O. In other words, all disk reads and writes are for one or more complete sectors. Sector sizes are typically a power of two, in the range 512 to 16K bytes, depending on the operating system and the size and speed of the disk drive. The block size used for external sorting algorithms should be equal to or a multiple of the sector size. Under this model, a sorting algorithm reads a block of data into a buffer in main memory, performs some processing on it, and at some future time writes it back to disk. From Section 8.1 we see that reading or writing a block from disk takes on the order of one million times longer than a memory access. Based on this fact, we can reasonably expect that the records contained in a single block can be sorted by an internal sorting algorithm such as Quicksort in less time than is required to read or write the block. Under good conditions, reading from a ﬁle in sequential order is more efﬁcient than reading blocks in random order. Given the signiﬁcant impact of seek time on disk access, it might seem obvious that sequential processing is faster. However, it is important to understand precisely under what circumstances sequential ﬁle processing is actually faster than random access, because it affects our approach to designing an external sorting algorithm. Efﬁcient sequential access relies on seek time being kept to a minimum. The ﬁrst requirement is that the blocks making up a ﬁle are in fact stored on disk in sequential order and close together, preferably ﬁlling a small number of contiguous tracks. At the very least, the number of extents making up the ﬁle should be small. Users typically do not have much control over the layout of their ﬁle on disk, but writing a ﬁle all at once in sequential order to a disk drive with a high percentage of free space increases the likelihood of such an arrangement. The second requirement is that the disk drive’s I/O head remain positioned over the ﬁle throughout sequential processing. This will not happen if there is competition of any kind for the I/O head. For example, on a multi-user time-shared computer the sorting process might compete for the I/O head with the processes of other users. Even when the sorting process has sole control of the I/O head, it is still likely that sequential processing will not be efﬁcient. Imagine the situation where all processing is done on a single disk drive, with the typical arrangement of a single bank of read/write heads that move together over a stack of platters. If the sorting process involves reading from an input ﬁle, alternated with writing to an output ﬁle, then the I/O head will continuously seek between the input ﬁle and the output ﬁle. Similarly, if two input ﬁles are being processed simultaneously (such as during a merge process), then the I/O head will continuously seek between these two ﬁles. Sec. 8.5 External Sorting 285 The moral is that, with a single disk drive, there often is no such thing as efﬁ- cient sequential processing of a data ﬁle. Thus, a sorting algorithm might be more efﬁcient if it performs a smaller number of non-sequential disk operations rather than a larger number of logically sequential disk operations that require a large number of seeks in practice. As mentioned previously, the record size might be quite large compared to the size of the key.Trees are typically used to or- ganize large databases that must support record insertion, deletion, and key range searches. Section 10.2 brieﬂy describes ISAM, a tentative step toward solving the problem of storing a large database that must support insertion and deletion of records. Its shortcomings help to illustrate the value of tree indexing techniques.274 Chap. 8 File Processing and External Sorting This example illustrates why it is important to keep disk ﬁles from be- coming fragmented, and why so-called “disk defragmenters” can speed up ﬁle processing time. File fragmentation happens most commonly when the disk is nearly full and the ﬁle manager must search for free space whenever a ﬁle is created or changed. 8.3 Buﬀers and Buﬀer Pools Given the speciﬁcations of the disk drive from Example 8.1, we ﬁnd that it takes about 9.5+11.1×1.5 = 26.2 ms to read one track of data on average. It takes about 9.5+11.1/2+(1/256)×11.1 = 15.1 ms on average to read a single sector of data. This is a good savings (slightly over half the time), but less than 1% of the data on the track are read. If we want to read only a single byte, it would save us effectively no time over that required to read an entire sector. For this reason, nearly all disk drives automatically read or write an entire sector’s worth of information whenever the disk is accessed, even when only one byte of information is requested. Once a sector is read, its information is stored in main memory. This is known as buffering or caching the information. If the next disk request is to that same sector, then it is not necessary to read from disk again because the information is already stored in main memory. Buffering is an example of one method for minimizing disk accesses mentioned at the beginning of the chapter: Bring off additional information from disk to satisfy future requests. If information from ﬁles were accessed at random, then the chance that two consecutive disk requests are to the same sector would be low. However, in practice most disk requests are close to the location (in the logical ﬁle at least) of the previous request. This means that the probability of the next request “hitting the cache” is much higher than chance would indicate. This principle explains one reason why average access times for new disk drives are lower than in the past. Not only is the hardware faster, but information is also now stored using better algorithms and larger caches that minimize the number of times information needs to be fetched from disk. This same concept is also used to store parts of programs in faster memory within the CPU, using the CPU cache that is prevalent in modern microprocessors. Sector-level buffering is normally provided by the operating system and is of- ten built directly into the disk drive controller hardware. Most operating systems maintain at least two buffers, one for input and one for output. Consider what would happen if there were only one buffer during a byte-by-byte copy operation. The sector containing the ﬁrst byte would be read into the I/O buffer. The output operation would need to destroy the contents of the single I/O buffer to write this byte. Then the buffer would need to be ﬁlled again from disk for the second byte,  Sec. 8.3 Buﬀers and Buﬀer Pools 275 only to be destroyed during output. The simple solution to this problem is to keep one buffer for input, and a second for output. Most disk drive controllers operate independently from the CPU once an I/O request is received. This is useful because the CPU can typically execute millions of instructions during the time required for a single I/O operation.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant to the input, addressing the request for key learning concepts without any irrelevant information. Great job!, error: None)\n",
      "  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because all nodes in the retrieval context are relevant and well-ranked. Great job on achieving perfect precision!, error: None)\n",
      "  - ❌ Contextual Recall (score: 0.3333333333333333, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.33 because only a portion of the expected output, specifically regarding non-recursive search algorithms, aligns with the content in the nodes in retrieval context. However, other topics like data structures, algorithm analysis, and various sorting algorithms are not reflected in the retrieval context., error: None)\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions. Great job on maintaining accuracy and consistency with the retrieval context!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Identify the 5 most important learning concepts for chapter Searching. The relevant context can be found here: We do not care right now what the precise value of c might be. Nor are we concerned with the time required to increment vari- able i because this must be done for each value in the array, or the time for the actual assignment when a larger value is found, or the little bit of extra time taken to initialize currlarge. We just want a reasonable ap- proximation for the time taken to execute the algorithm. The total time to run largest is therefore approximately cn, because we must make n comparisons, with each comparison costing c time. We say that function largest (and by extension ,the largest-value sequential search algorithm for any typical implementation) has a running time expressed by the equa- tion T(n) = cn. This equation describes the growth rate for the running time of the largest- value sequential search algorithm. Example 3.2 The running time of a statement that assigns the ﬁrst value of an integer array to a variable is simply the time required to copy the value of the ﬁrst array value. We can assume this assignment takes a constant amount of time regardless of the value. Let us call c1 the amount of time necessary to copy an integer. No matter how large the array on a typical computer (given reasonable conditions for memory and array size), the time to copy the value from the ﬁrst position of the array is always c1. Thus, the equation for this algorithm is simply T(n) = c1, indicating that the size of the input n has no effect on the running time. This is called a constant running time. Example 3.3 Consider the following code: sum = 0; for (i=1; i<=n; i++) for (j=1; j<=n; j++) sum++; What is the running time for this code fragment? Clearly it takes longer to run when n is larger. The basic operation in this example is the increment  Sec. 3.1 Introduction 57 0 100 200 300 400 10n 20n 2n2 5n log n 2n n!Sequential search on an unsorted list requires Θ(n) time in the worst case. How many comparisons does linear search do on average?For this example, the expected number of accesses is a constant. This is because the probability for accessing the ﬁrst record is high (one half), the second is much lower (one quarter) but still much higher than for the third record, and so on. This shows that for some probability distributions, or- dering the list by frequency can yield an efﬁcient search technique. In many search applications, real access patterns follow a rule of thumb called the 80/20 rule. The 80/20 rule says that 80% of the record accesses are to 20% of the records. The values of 80 and 20 are only estimates; every data access pat- tern has its own values. However, behavior of this nature occurs surprisingly often in practice (which explains the success of caching techniques widely used by web browsers for speeding access to web pages, and by disk drive and CPU manufac- turers for speeding access to data stored in slower memory; see the discussion on buffer pools in Section 8.3). When the 80/20 rule applies, we can expect consid- erable improvements to search performance from a list ordered by frequency of access over standard sequential search in an unordered list. Example 9.3 The 80/20 rule is an example of a Zipf distribution. Nat- urally occurring distributions often follow a Zipf distribution. Examples include the observed frequency for the use of words in a natural language such as English, and the size of the population for cities (i.e., view the relative proportions for the populations as equivalent to the “frequency of use”). Zipf distributions are related to the Harmonic Series deﬁned in Equa- tion 2.10.The classic example is a large database of records with multiple search keys, requiring the ability to insert, delete, and search for records. Hashing provides outstanding performance for such situations, but only in the limited case in which all searches are of the form “ﬁnd the record with key value K.” Many applications require more general search capabilities. One exam- ple is a range query search for all records whose key lies within some range. Other queries might involve visiting all records in order of their key value, or ﬁnding the record with the greatest key value. Hash tables are not organized to support any of these queries efﬁciently. This chapter introduces ﬁle structures used to organize a large collection of records stored on disk. Such ﬁle structures support efﬁcient insertion, deletion, and search operations, for exact-match queries, range queries, and largest/smallest key value searches. Before discussing such ﬁle structures, we must become familiar with some ba- sic ﬁle-processing terminology. An entry-sequenced ﬁle stores records in the order that they were added to the ﬁle. Entry-sequenced ﬁles are the disk-based equivalent to an unsorted list and so do not support efﬁcient search. The natural solution is to sort the records by order of the search key. However, a typical database, such as a collection of employee or customer records maintained by a business, might con- tain multiple search keys. To answer a question about a particular customer might require a search on the name of the customer. Businesses often wish to sort and output the records by zip code order for a bulk mailing. Government paperwork might require the ability to search by Social Security number. Thus, there might not be a single “correct” order in which to store the records. Indexing is the process of associating a key with the location of a correspond- ing data record. Section 8.5 discussed the concept of a key sort, in which an index ﬁle is created whose records consist of key/pointer pairs. Here, each key is asso- ciated with a pointer to a complete record in the main database ﬁle. The index ﬁle 341  342 Chap. 10 Indexing could be sorted or organized using a tree structure, thereby imposing a logical or- der on the records without physically rearranging them. One database might have several associated index ﬁles, each supporting efﬁcient access through a different key ﬁeld. Each record of a database normally has a unique identiﬁer, called the primary key. For example, the primary key for a set of personnel records might be the Social Security number or ID number for the individual. Unfortunately, the ID number is generally an inconvenient value on which to perform a search because the searcher is unlikely to know it. Instead, the searcher might know the desired employee’s name. Alternatively, the searcher might be interested in ﬁnding all employees whose salary is in a certain range. If these are typical search requests to the database, then the name and salary ﬁelds deserve separate indices. However, key values in the name and salary indices are not likely to be unique. A key ﬁeld such as salary, where a particular key value might be duplicated in multiple records, is called a secondary key. Most searches are performed using a secondary key. The secondary key index (or more simply, secondary index) will associate a secondary key value with the primary key of each record having that secondary key value. At this point, the full database might be searched directly for the record with that primary key, or there might be a primary key index (or primary index) that relates each primary key value with a pointer to the actual record on disk. In the latter case, only the primary index provides the location of the actual record on disk, while the secondary indices refer to the primary index. Indexing is an important technique for organizing large databases, and many indexing methods have been developed. Direct access through hashing is discussed in Section 9.4. A simple list sorted by key value can also serve as an index to the record ﬁle. Indexing disk ﬁles by sorted lists are discussed in the following section. Unfortunately, a sorted list does not perform well for insert and delete operations. A third approach to indexing is the tree index..\n",
      "  - actual output: Based on the provided context regarding the chapter on Searching, here are the five most important learning concepts: 1. **Time Complexity of Search Algorithms**:    - Understanding the time complexity of different search algorithms is crucial. For example, the sequential search algorithm has a running time of T(n) = cn, indicating that the time taken grows linearly with the number of elements (n) in the array. This concept helps in evaluating the efficiency of search algorithms. 2. **Constant vs. Linear Time**:    - The distinction between constant time (T(n) = c1) and linear time (T(n) = cn) is important. Constant time indicates that the execution time does not depend on the size of the input, while linear time indicates that the execution time increases proportionally with the input size. This understanding is fundamental when analyzing algorithm performance. 3. **The 80/20 Rule and Access Patterns**:    - The 80/20 rule, which states that 80% of record accesses are to 20% of the records, highlights the importance of understanding access patterns in search operations. This concept can lead to optimizations, such as ordering data by frequency of access to improve search performance. 4. **Indexing and Key Structures**:    - Indexing is a critical technique for organizing large databases to facilitate efficient search operations. Understanding the difference between primary and secondary keys, as well as how indices can be structured (e.g., sorted lists, tree indices), is essential for optimizing search queries in databases. 5. **Hashing and Search Capabilities**:    - Hashing provides a method for efficient searching, particularly for exact-match queries. However, it is important to recognize its limitations for more complex queries, such as range queries. Understanding when to use hashing versus other indexing methods is key to effective database management and search optimization. These concepts form the foundation for understanding searching algorithms and their applications in data structures and databases.\n",
      "  - expected output: Data Structures Basics Algorithm Analysis O Notation Algorithms Non-recursive Algorthims Non-recursive Algorithms Search Linear Search Binary Search Sort Insertion Sort Selection Sort Bubble Sort Recursive Algorithms Recursive Binary Search Recursive Sort Merge Sort Quick Sort\n",
      "  - context: None\n",
      "  - retrieval context: ['We do not care right now what the precise value of c might be. Nor are we concerned with the time required to increment vari- able i because this must be done for each value in the array, or the time for the actual assignment when a larger value is found, or the little bit of extra time taken to initialize currlarge. We just want a reasonable ap- proximation for the time taken to execute the algorithm. The total time to run largest is therefore approximately cn, because we must make n comparisons, with each comparison costing c time. We say that function largest (and by extension ,the largest-value sequential search algorithm for any typical implementation) has a running time expressed by the equa- tion T(n) = cn. This equation describes the growth rate for the running time of the largest- value sequential search algorithm. Example 3.2 The running time of a statement that assigns the ﬁrst value of an integer array to a variable is simply the time required to copy the value of the ﬁrst array value. We can assume this assignment takes a constant amount of time regardless of the value. Let us call c1 the amount of time necessary to copy an integer. No matter how large the array on a typical computer (given reasonable conditions for memory and array size), the time to copy the value from the ﬁrst position of the array is always c1. Thus, the equation for this algorithm is simply T(n) = c1, indicating that the size of the input n has no effect on the running time. This is called a constant running time. Example 3.3 Consider the following code: sum = 0; for (i=1; i<=n; i++) for (j=1; j<=n; j++) sum++; What is the running time for this code fragment? Clearly it takes longer to run when n is larger. The basic operation in this example is the increment  Sec. 3.1 Introduction 57 0 100 200 300 400 10n 20n 2n2 5n log n 2n n!Sequential search on an unsorted list requires Θ(n) time in the worst case. How many comparisons does linear search do on average?For this example, the expected number of accesses is a constant. This is because the probability for accessing the ﬁrst record is high (one half), the second is much lower (one quarter) but still much higher than for the third record, and so on. This shows that for some probability distributions, or- dering the list by frequency can yield an efﬁcient search technique. In many search applications, real access patterns follow a rule of thumb called the 80/20 rule. The 80/20 rule says that 80% of the record accesses are to 20% of the records. The values of 80 and 20 are only estimates; every data access pat- tern has its own values. However, behavior of this nature occurs surprisingly often in practice (which explains the success of caching techniques widely used by web browsers for speeding access to web pages, and by disk drive and CPU manufac- turers for speeding access to data stored in slower memory; see the discussion on buffer pools in Section 8.3). When the 80/20 rule applies, we can expect consid- erable improvements to search performance from a list ordered by frequency of access over standard sequential search in an unordered list. Example 9.3 The 80/20 rule is an example of a Zipf distribution. Nat- urally occurring distributions often follow a Zipf distribution. Examples include the observed frequency for the use of words in a natural language such as English, and the size of the population for cities (i.e., view the relative proportions for the populations as equivalent to the “frequency of use”). Zipf distributions are related to the Harmonic Series deﬁned in Equa- tion 2.10.The classic example is a large database of records with multiple search keys, requiring the ability to insert, delete, and search for records. Hashing provides outstanding performance for such situations, but only in the limited case in which all searches are of the form “ﬁnd the record with key value K.” Many applications require more general search capabilities. One exam- ple is a range query search for all records whose key lies within some range. Other queries might involve visiting all records in order of their key value, or ﬁnding the record with the greatest key value. Hash tables are not organized to support any of these queries efﬁciently. This chapter introduces ﬁle structures used to organize a large collection of records stored on disk. Such ﬁle structures support efﬁcient insertion, deletion, and search operations, for exact-match queries, range queries, and largest/smallest key value searches. Before discussing such ﬁle structures, we must become familiar with some ba- sic ﬁle-processing terminology. An entry-sequenced ﬁle stores records in the order that they were added to the ﬁle. Entry-sequenced ﬁles are the disk-based equivalent to an unsorted list and so do not support efﬁcient search. The natural solution is to sort the records by order of the search key. However, a typical database, such as a collection of employee or customer records maintained by a business, might con- tain multiple search keys. To answer a question about a particular customer might require a search on the name of the customer. Businesses often wish to sort and output the records by zip code order for a bulk mailing. Government paperwork might require the ability to search by Social Security number. Thus, there might not be a single “correct” order in which to store the records. Indexing is the process of associating a key with the location of a correspond- ing data record. Section 8.5 discussed the concept of a key sort, in which an index ﬁle is created whose records consist of key/pointer pairs. Here, each key is asso- ciated with a pointer to a complete record in the main database ﬁle. The index ﬁle 341  342 Chap. 10 Indexing could be sorted or organized using a tree structure, thereby imposing a logical or- der on the records without physically rearranging them. One database might have several associated index ﬁles, each supporting efﬁcient access through a different key ﬁeld. Each record of a database normally has a unique identiﬁer, called the primary key. For example, the primary key for a set of personnel records might be the Social Security number or ID number for the individual. Unfortunately, the ID number is generally an inconvenient value on which to perform a search because the searcher is unlikely to know it. Instead, the searcher might know the desired employee’s name. Alternatively, the searcher might be interested in ﬁnding all employees whose salary is in a certain range. If these are typical search requests to the database, then the name and salary ﬁelds deserve separate indices. However, key values in the name and salary indices are not likely to be unique. A key ﬁeld such as salary, where a particular key value might be duplicated in multiple records, is called a secondary key. Most searches are performed using a secondary key. The secondary key index (or more simply, secondary index) will associate a secondary key value with the primary key of each record having that secondary key value. At this point, the full database might be searched directly for the record with that primary key, or there might be a primary key index (or primary index) that relates each primary key value with a pointer to the actual record on disk. In the latter case, only the primary index provides the location of the actual record on disk, while the secondary indices refer to the primary index. Indexing is an important technique for organizing large databases, and many indexing methods have been developed. Direct access through hashing is discussed in Section 9.4. A simple list sorted by key value can also serve as an index to the record ﬁle. Indexing disk ﬁles by sorted lists are discussed in the following section. Unfortunately, a sorted list does not perform well for insert and delete operations. A third approach to indexing is the tree index.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the request without any irrelevant statements. Great job!, error: None)\n",
      "  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the relevant nodes discussing 'various sorting algorithms including Quicksort and Mergesort' are appropriately ranked and there are no irrelevant nodes above them. Great job in maintaining precision!, error: None)\n",
      "  - ✅ Contextual Recall (score: 0.5, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.50 because only part of the expected output (related to sorting algorithms and recursive algorithms) is supported by the retrieval context, specifically nodes 1 and 2, while other topics like data structures, algorithm analysis, and search algorithms are not covered in the context., error: None)\n",
      "  - ✅ Faithfulness (score: 0.8888888888888888, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.89 because the actual output incorrectly states that the theoretical lower bound for sorting algorithms is Ω(n log n) in the worst case, while this bound is actually for the average case., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Identify the 5 most important learning concepts for chapter Internal Sorting. The relevant context can be found here: We must be careful about how we interpret this last statement, however. The world is certainly better off for the invention of Quicksort, even though Mergesort was available at the time. Quicksort is not asymptotically faster than Mergesort, yet is not merely a “tuning” of Mergesort either. Quicksort is a substantially different approach to sorting. So even when our upper and lower bounds for a problem meet, there are still beneﬁts to be gained from a new, clever algorithm.7.5 Quicksort 241 is still unlikely to happen. It does not take many good partitionings for Quicksort to work fairly well. Quicksort’s best case occurs when findpivot always breaks the array into two equal halves. Quicksort repeatedly splits the array into smaller partitions, as shown in Figure 7.14. In the best case, the result will be log n levels of partitions, with the top level having one array of size n, the second level two arrays of size n/2, the next with four arrays of size n/4, and so on. Thus, at each level, all partition steps for that level do a total of n work, for an overall cost of n log n work when Quicksort ﬁnds perfect pivots. Quicksort’s average-case behavior falls somewhere between the extremes of worst and best case. Average-case analysis considers the cost for all possible ar- rangements of input, summing the costs and dividing by the number of cases. We make one reasonable simplifying assumption: At each partition step, the pivot is equally likely to end in any position in the (sorted) array. In other words, the pivot is equally likely to break an array into partitions of sizes 0 and n−1, or 1 and n−2, and so on. Given this assumption, the average-case cost is computed from the following equation: T(n) = cn + 1 n n−1 X k=0 [T(k) + T(n −1 −k)], T(0) = T(1) = c. This equation is in the form of a recurrence relation.If not, we can solve the subproblem recursively by only consid- ering one of the sublists. That is, if the pivot ends up in position k > i, then we  500 Chap. 15 Lower Bounds Figure 15.5 A method for ﬁnding a pivot for partitioning a list that guarantees at least a ﬁxed fraction of the list will be in each partition. We divide the list into groups of ﬁve elements, and ﬁnd the median for each group. We then recursively ﬁnd the median of these n/5 medians. The median of ﬁve elements is guaran- teed to have at least two in each partition. The median of three medians from a collection of 15 elements is guaranteed to have at least ﬁve elements in each partition. simply solve by ﬁnding the ith best element in the left partition. If the pivot is at position k < i, then we wish to ﬁnd the i −kth element in the right partition. What is the worst case cost of this algorithm? As with Quicksort, we get bad performance if the pivot is the ﬁrst or last element in the array. This would lead to possibly O(n2) performance. However, if the pivot were to always cut the array in half, then our cost would be modeled by the recurrence T(n) = T(n/2) +n = 2n or O(n) cost. Finding the average cost requires us to use a recurrence with full history, similar to the one we used to model the cost of Quicksort. If we do this, we will ﬁnd that T(n) is in O(n) in the average case. Is it possible to modify our algorithm to get worst-case linear time? To do this, we need to pick a pivot that is guaranteed to discard a ﬁxed fraction of the elements. We cannot just choose a pivot at random, because doing so will not meet this guarantee. The ideal situation would be if we could pick the median value for the pivot each time. But that is essentially the same problem that we are trying to solve to begin with. Notice, however, that if we choose any constant c, and then if we pick the median from a sample of size n/c, then we can guarantee that we will discard at least n/2c elements. Actually, we can do better than this by selecting small subsets of a constant size (so we can ﬁnd the median of each in constant time), and then taking the median of these medians. Figure 15.5 illustrates this idea. This observation leads directly to the following algorithm. • Choose the n/5 medians for groups of ﬁve elements from the list. Choosing the median of ﬁve items can be done in constant time. • Recursively, select M, the median of the n/5 medians-of-ﬁves. • Partition the list into those elements larger and smaller than M. Sec. 15.7 Optimal Sorting 501 While selecting the median in this way is guaranteed to eliminate a fraction of the elements (leaving at most ⌈(7n −5)/10⌉elements left), we still need to be sure that our recursion yields a linear-time algorithm. We model the algorithm by the following recurrence. T(n) ≤T(⌈n/5⌉) + T(⌈(7n −5)/10⌉) + 6⌈n/5⌉+ n −1. The T(⌈n/5⌉) term comes from computing the median of the medians-of-ﬁves, the 6⌈n/5⌉term comes from the cost to calculate the median-of-ﬁves (exactly six comparisons for each group of ﬁve element), and the T(⌈(7n−5)/10⌉) term comes from the recursive call of the remaining (up to) 70% of the elements that might be left. We will prove that this recurrence is linear by assuming that it is true for some constant r, and then show that T(n) ≤rn for all n greater than some bound. T(n) ≤ T(⌈n 5 ⌉) + T(⌈7n −5 10 ⌉) + 6⌈n 5 ⌉+ n −1 ≤ r(n 5 + 1) + r(7n −5 10 + 1) + 6(n 5 + 1) + n −1 ≤ (r 5 + 7r 10 + 11 5 )n + 3r 2 + 5 ≤ 9r + 22 10 n + 3r + 10 2 . This is true for r ≥23 and n ≥380. This provides a base case that allows us to use induction to prove that ∀n ≥380, T(n) ≤23n. In reality, this algorithm is not practical because its constant factor costs are so high.Some of these algorithms are straightforward adaptations of schemes we use in everyday life. Others are totally alien to how hu- mans do things, having been invented to sort thousands or even millions of records stored on the computer. After years of study, there are still unsolved problems related to sorting. New algorithms are still being developed and reﬁned for special- purpose applications. While introducing this central problem in computer science, this chapter has a secondary purpose of illustrating issues in algorithm design and analysis. For example, this collection of sorting algorithms shows multiple approaches to us- ing divide-and-conquer. In particular, there are multiple ways to do the dividing: Mergesort divides a list in half; Quicksort divides a list into big values and small values; and Radix Sort divides the problem by working on one digit of the key at a time. Sorting algorithms can also illustrate a wide variety of analysis techniques. We’ll ﬁnd that it is possible for an algorithm to have an average case whose growth rate is signiﬁcantly smaller than its worse case (Quicksort). We’ll see how it is possible to speed up sorting algorithms (both Shellsort and Quicksort) by taking advantage of the best case behavior of another algorithm (Insertion sort). We’ll see several examples of how we can tune an algorithm for better performance. We’ll see that special case behavior by some algorithms makes them a good solution for 223  224 Chap. 7 Internal Sorting special niche applications (Heapsort). Sorting provides an example of a signiﬁcant technique for analyzing the lower bound for a problem. Sorting will also be used to motivate the introduction to ﬁle processing presented in Chapter 8. The present chapter covers several standard algorithms appropriate for sorting a collection of records that ﬁt in the computer’s main memory. It begins with a dis- cussion of three simple, but relatively slow, algorithms requiring Θ(n2) time in the average and worst cases. Several algorithms with considerably better performance are then presented, some with Θ(n log n) worst-case running time. The ﬁnal sort- ing method presented requires only Θ(n) worst-case time under special conditions. The chapter concludes with a proof that sorting in general requires Ω(n log n) time in the worst case. 7.1 Sorting Terminology and Notation Except where noted otherwise, input to the sorting algorithms presented in this chapter is a collection of records stored in an array. Records are compared to one another by requiring that their type extend the Comparable class. This will ensure that the class implements the compareTo method, which returns a value less than zero, equal to zero, or greater than zero depending on its relationship to the record being compared to. The compareTo method is deﬁned to extract the appropriate key ﬁeld from the record. We also assume that for every record type there is a swap function that can interchange the contents of two records in the array. Given a set of records r1, r2, ..., rn with key values k1, k2, ..., kn, the Sorting Problem is to arrange the records into any order s such that records rs1, rs2, ..., rsn have keys obeying the property ks1 ≤ks2 ≤... ≤ksn. In other words, the sorting problem is to arrange a set of records so that the values of their key ﬁelds are in non-decreasing order. As deﬁned, the Sorting Problem allows input with two or more records that have the same key value. Certain applications require that input not contain duplicate key values. The sorting algorithms presented in this chapter and in Chapter 8 can handle duplicate key values unless noted otherwise. When duplicate key values are allowed, there might be an implicit ordering to the duplicates, typically based on their order of occurrence within the input. It might be desirable to maintain this initial ordering among duplicates. A sorting algorithm is said to be stable if it does not change the relative ordering of records with identical key values. Many, but not all, of the sorting algorithms presented in this chapter are stable, or can be made stable with minor changes. When comparing two sorting algorithms, the most straightforward approach would seem to be simply program both and measure their running times. An ex- ample of such timings is presented in Figure 7.20. However, such a comparison  Sec. 7.2 Three Θ(n2) Sorting Algorithms 225 can be misleading because the running time for many sorting algorithms depends on speciﬁcs of the input values. In particular, the number of records, the size of the keys and the records, the allowable range of the key values, and the amount by which the input records are “out of order” can all greatly affect the relative running times for sorting algorithms. When analyzing sorting algorithms, it is traditional to measure the number of comparisons made between keys. This measure is usually closely related to the running time for the algorithm and has the advantage of being machine and data- type independent. However, in some cases records might be so large that their physical movement might take a signiﬁcant fraction of the total running time. If so, it might be appropriate to measure the number of swap operations performed by the algorithm. In most applications we can assume that all records and keys are of ﬁxed length, and that a single comparison or a single swap operation requires a constant amount of time regardless of which keys are involved. Some special situations “change the rules” for comparing sorting algorithms. For example, an application with records or keys having widely varying length (such as sorting a sequence of variable length strings) will beneﬁt from a special-purpose sorting technique. Some applications require that a small number of records be sorted, but that the sort be performed frequently. An example would be an application that repeatedly sorts groups of ﬁve numbers. In such cases, the constants in the runtime equations that are usually ignored in an asymptotic analysis now become crucial. Finally, some situations require that a sorting algorithm use as little memory as possible. We will note which sorting algorithms require signiﬁcant extra memory beyond the input array. 7.2 Three Θ(n2) Sorting Algorithms This section presents three simple sorting algorithms. While easy to understand and implement, we will soon see that they are unacceptably slow when there are many records to sort..\n",
      "  - actual output: Here are the five most important learning concepts from the chapter on Internal Sorting, particularly focusing on the discussion of sorting algorithms like Quicksort and Mergesort: 1. **Algorithm Efficiency and Complexity**:    - Understanding the time complexity of sorting algorithms is crucial. The chapter discusses various sorting algorithms, highlighting their average, best, and worst-case performance. For instance, Quicksort has an average-case time complexity of O(n log n) but can degrade to O(n²) in the worst case if poor pivot choices are made. This concept emphasizes the importance of analyzing and comparing the efficiency of different algorithms. 2. **Divide-and-Conquer Strategy**:    - Many sorting algorithms, including Quicksort and Mergesort, utilize the divide-and-conquer approach. This involves breaking down a problem into smaller subproblems, solving each subproblem independently, and then combining the results. Understanding this strategy is essential for grasping how these algorithms operate and their underlying principles. 3. **Pivot Selection in Quicksort**:    - The choice of pivot in Quicksort significantly affects its performance. The best-case scenario occurs when the pivot divides the array into two equal halves, leading to optimal performance. The chapter also discusses strategies for improving pivot selection to avoid worst-case scenarios, such as using the median of medians to ensure a more balanced partitioning. 4. **Stability in Sorting Algorithms**:    - The concept of stability in sorting algorithms is introduced, where a stable sorting algorithm maintains the relative order of records with equal keys. This is important in applications where the order of records matters. The chapter notes that while some algorithms are inherently stable, others can be modified to achieve stability. 5. **Lower Bound for Sorting**:    - The chapter concludes with a discussion on the theoretical lower bound for sorting algorithms, which is Ω(n log n) in the worst case. This concept is fundamental in understanding the limitations of sorting algorithms and the inherent complexity of the sorting problem, regardless of the algorithm used. These concepts provide a foundational understanding of sorting algorithms, their analysis, and their practical implications in computer science.\n",
      "  - expected output: Data Structures Basics Algorithm Analysis O Notation Algorithms Non-recursive Algorthims Non-recursive Algorithms Search Linear Search Binary Search Sort Insertion Sort Selection Sort Bubble Sort Recursive Algorithms Recursive Binary Search Recursive Sort Merge Sort Quick Sort\n",
      "  - context: None\n",
      "  - retrieval context: ['We must be careful about how we interpret this last statement, however. The world is certainly better off for the invention of Quicksort, even though Mergesort was available at the time. Quicksort is not asymptotically faster than Mergesort, yet is not merely a “tuning” of Mergesort either. Quicksort is a substantially different approach to sorting. So even when our upper and lower bounds for a problem meet, there are still beneﬁts to be gained from a new, clever algorithm.7.5 Quicksort 241 is still unlikely to happen. It does not take many good partitionings for Quicksort to work fairly well. Quicksort’s best case occurs when findpivot always breaks the array into two equal halves. Quicksort repeatedly splits the array into smaller partitions, as shown in Figure 7.14. In the best case, the result will be log n levels of partitions, with the top level having one array of size n, the second level two arrays of size n/2, the next with four arrays of size n/4, and so on. Thus, at each level, all partition steps for that level do a total of n work, for an overall cost of n log n work when Quicksort ﬁnds perfect pivots. Quicksort’s average-case behavior falls somewhere between the extremes of worst and best case. Average-case analysis considers the cost for all possible ar- rangements of input, summing the costs and dividing by the number of cases. We make one reasonable simplifying assumption: At each partition step, the pivot is equally likely to end in any position in the (sorted) array. In other words, the pivot is equally likely to break an array into partitions of sizes 0 and n−1, or 1 and n−2, and so on. Given this assumption, the average-case cost is computed from the following equation: T(n) = cn + 1 n n−1 X k=0 [T(k) + T(n −1 −k)], T(0) = T(1) = c. This equation is in the form of a recurrence relation.If not, we can solve the subproblem recursively by only consid- ering one of the sublists. That is, if the pivot ends up in position k > i, then we  500 Chap. 15 Lower Bounds Figure 15.5 A method for ﬁnding a pivot for partitioning a list that guarantees at least a ﬁxed fraction of the list will be in each partition. We divide the list into groups of ﬁve elements, and ﬁnd the median for each group. We then recursively ﬁnd the median of these n/5 medians. The median of ﬁve elements is guaran- teed to have at least two in each partition. The median of three medians from a collection of 15 elements is guaranteed to have at least ﬁve elements in each partition. simply solve by ﬁnding the ith best element in the left partition. If the pivot is at position k < i, then we wish to ﬁnd the i −kth element in the right partition. What is the worst case cost of this algorithm? As with Quicksort, we get bad performance if the pivot is the ﬁrst or last element in the array. This would lead to possibly O(n2) performance. However, if the pivot were to always cut the array in half, then our cost would be modeled by the recurrence T(n) = T(n/2) +n = 2n or O(n) cost. Finding the average cost requires us to use a recurrence with full history, similar to the one we used to model the cost of Quicksort. If we do this, we will ﬁnd that T(n) is in O(n) in the average case. Is it possible to modify our algorithm to get worst-case linear time? To do this, we need to pick a pivot that is guaranteed to discard a ﬁxed fraction of the elements. We cannot just choose a pivot at random, because doing so will not meet this guarantee. The ideal situation would be if we could pick the median value for the pivot each time. But that is essentially the same problem that we are trying to solve to begin with. Notice, however, that if we choose any constant c, and then if we pick the median from a sample of size n/c, then we can guarantee that we will discard at least n/2c elements. Actually, we can do better than this by selecting small subsets of a constant size (so we can ﬁnd the median of each in constant time), and then taking the median of these medians. Figure 15.5 illustrates this idea. This observation leads directly to the following algorithm. • Choose the n/5 medians for groups of ﬁve elements from the list. Choosing the median of ﬁve items can be done in constant time. • Recursively, select M, the median of the n/5 medians-of-ﬁves. • Partition the list into those elements larger and smaller than M. Sec. 15.7 Optimal Sorting 501 While selecting the median in this way is guaranteed to eliminate a fraction of the elements (leaving at most ⌈(7n −5)/10⌉elements left), we still need to be sure that our recursion yields a linear-time algorithm. We model the algorithm by the following recurrence. T(n) ≤T(⌈n/5⌉) + T(⌈(7n −5)/10⌉) + 6⌈n/5⌉+ n −1. The T(⌈n/5⌉) term comes from computing the median of the medians-of-ﬁves, the 6⌈n/5⌉term comes from the cost to calculate the median-of-ﬁves (exactly six comparisons for each group of ﬁve element), and the T(⌈(7n−5)/10⌉) term comes from the recursive call of the remaining (up to) 70% of the elements that might be left. We will prove that this recurrence is linear by assuming that it is true for some constant r, and then show that T(n) ≤rn for all n greater than some bound. T(n) ≤ T(⌈n 5 ⌉) + T(⌈7n −5 10 ⌉) + 6⌈n 5 ⌉+ n −1 ≤ r(n 5 + 1) + r(7n −5 10 + 1) + 6(n 5 + 1) + n −1 ≤ (r 5 + 7r 10 + 11 5 )n + 3r 2 + 5 ≤ 9r + 22 10 n + 3r + 10 2 . This is true for r ≥23 and n ≥380. This provides a base case that allows us to use induction to prove that ∀n ≥380, T(n) ≤23n. In reality, this algorithm is not practical because its constant factor costs are so high.Some of these algorithms are straightforward adaptations of schemes we use in everyday life. Others are totally alien to how hu- mans do things, having been invented to sort thousands or even millions of records stored on the computer. After years of study, there are still unsolved problems related to sorting. New algorithms are still being developed and reﬁned for special- purpose applications. While introducing this central problem in computer science, this chapter has a secondary purpose of illustrating issues in algorithm design and analysis. For example, this collection of sorting algorithms shows multiple approaches to us- ing divide-and-conquer. In particular, there are multiple ways to do the dividing: Mergesort divides a list in half; Quicksort divides a list into big values and small values; and Radix Sort divides the problem by working on one digit of the key at a time. Sorting algorithms can also illustrate a wide variety of analysis techniques. We’ll ﬁnd that it is possible for an algorithm to have an average case whose growth rate is signiﬁcantly smaller than its worse case (Quicksort). We’ll see how it is possible to speed up sorting algorithms (both Shellsort and Quicksort) by taking advantage of the best case behavior of another algorithm (Insertion sort). We’ll see several examples of how we can tune an algorithm for better performance. We’ll see that special case behavior by some algorithms makes them a good solution for 223  224 Chap. 7 Internal Sorting special niche applications (Heapsort). Sorting provides an example of a signiﬁcant technique for analyzing the lower bound for a problem. Sorting will also be used to motivate the introduction to ﬁle processing presented in Chapter 8. The present chapter covers several standard algorithms appropriate for sorting a collection of records that ﬁt in the computer’s main memory. It begins with a dis- cussion of three simple, but relatively slow, algorithms requiring Θ(n2) time in the average and worst cases. Several algorithms with considerably better performance are then presented, some with Θ(n log n) worst-case running time. The ﬁnal sort- ing method presented requires only Θ(n) worst-case time under special conditions. The chapter concludes with a proof that sorting in general requires Ω(n log n) time in the worst case. 7.1 Sorting Terminology and Notation Except where noted otherwise, input to the sorting algorithms presented in this chapter is a collection of records stored in an array. Records are compared to one another by requiring that their type extend the Comparable class. This will ensure that the class implements the compareTo method, which returns a value less than zero, equal to zero, or greater than zero depending on its relationship to the record being compared to. The compareTo method is deﬁned to extract the appropriate key ﬁeld from the record. We also assume that for every record type there is a swap function that can interchange the contents of two records in the array. Given a set of records r1, r2, ..., rn with key values k1, k2, ..., kn, the Sorting Problem is to arrange the records into any order s such that records rs1, rs2, ..., rsn have keys obeying the property ks1 ≤ks2 ≤... ≤ksn. In other words, the sorting problem is to arrange a set of records so that the values of their key ﬁelds are in non-decreasing order. As deﬁned, the Sorting Problem allows input with two or more records that have the same key value. Certain applications require that input not contain duplicate key values. The sorting algorithms presented in this chapter and in Chapter 8 can handle duplicate key values unless noted otherwise. When duplicate key values are allowed, there might be an implicit ordering to the duplicates, typically based on their order of occurrence within the input. It might be desirable to maintain this initial ordering among duplicates. A sorting algorithm is said to be stable if it does not change the relative ordering of records with identical key values. Many, but not all, of the sorting algorithms presented in this chapter are stable, or can be made stable with minor changes. When comparing two sorting algorithms, the most straightforward approach would seem to be simply program both and measure their running times. An ex- ample of such timings is presented in Figure 7.20. However, such a comparison  Sec. 7.2 Three Θ(n2) Sorting Algorithms 225 can be misleading because the running time for many sorting algorithms depends on speciﬁcs of the input values. In particular, the number of records, the size of the keys and the records, the allowable range of the key values, and the amount by which the input records are “out of order” can all greatly affect the relative running times for sorting algorithms. When analyzing sorting algorithms, it is traditional to measure the number of comparisons made between keys. This measure is usually closely related to the running time for the algorithm and has the advantage of being machine and data- type independent. However, in some cases records might be so large that their physical movement might take a signiﬁcant fraction of the total running time. If so, it might be appropriate to measure the number of swap operations performed by the algorithm. In most applications we can assume that all records and keys are of ﬁxed length, and that a single comparison or a single swap operation requires a constant amount of time regardless of which keys are involved. Some special situations “change the rules” for comparing sorting algorithms. For example, an application with records or keys having widely varying length (such as sorting a sequence of variable length strings) will beneﬁt from a special-purpose sorting technique. Some applications require that a small number of records be sorted, but that the sort be performed frequently. An example would be an application that repeatedly sorts groups of ﬁve numbers. In such cases, the constants in the runtime equations that are usually ignored in an asymptotic analysis now become crucial. Finally, some situations require that a sorting algorithm use as little memory as possible. We will note which sorting algorithms require signiﬁcant extra memory beyond the input array. 7.2 Three Θ(n2) Sorting Algorithms This section presents three simple sorting algorithms. While easy to understand and implement, we will soon see that they are unacceptably slow when there are many records to sort.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant to the input, with no irrelevant statements present. Great job!, error: None)\n",
      "  - ❌ Contextual Precision (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the only node in the retrieval context is irrelevant to the input. The node does not focus on 'the learning concepts for chapter Indexing', but instead discusses 'indexing within databases, discussing file structures, search keys, and indexing methods'. This emphasizes the mismatch as it does not address the desired learning concepts, which results in a low score., error: None)\n",
      "  - ❌ Contextual Recall (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because none of the sentences in the expected output align with the nodes in the retrieval context, which is focused on database indexing rather than data structures or algorithms., error: None)\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions, indicating the actual output is perfectly aligned with the retrieval context. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Identify the 5 most important learning concepts for chapter Indexing. The relevant context can be found here: The classic example is a large database of records with multiple search keys, requiring the ability to insert, delete, and search for records. Hashing provides outstanding performance for such situations, but only in the limited case in which all searches are of the form “ﬁnd the record with key value K.” Many applications require more general search capabilities. One exam- ple is a range query search for all records whose key lies within some range. Other queries might involve visiting all records in order of their key value, or ﬁnding the record with the greatest key value. Hash tables are not organized to support any of these queries efﬁciently. This chapter introduces ﬁle structures used to organize a large collection of records stored on disk. Such ﬁle structures support efﬁcient insertion, deletion, and search operations, for exact-match queries, range queries, and largest/smallest key value searches. Before discussing such ﬁle structures, we must become familiar with some ba- sic ﬁle-processing terminology. An entry-sequenced ﬁle stores records in the order that they were added to the ﬁle. Entry-sequenced ﬁles are the disk-based equivalent to an unsorted list and so do not support efﬁcient search. The natural solution is to sort the records by order of the search key. However, a typical database, such as a collection of employee or customer records maintained by a business, might con- tain multiple search keys. To answer a question about a particular customer might require a search on the name of the customer. Businesses often wish to sort and output the records by zip code order for a bulk mailing. Government paperwork might require the ability to search by Social Security number. Thus, there might not be a single “correct” order in which to store the records. Indexing is the process of associating a key with the location of a correspond- ing data record. Section 8.5 discussed the concept of a key sort, in which an index ﬁle is created whose records consist of key/pointer pairs. Here, each key is asso- ciated with a pointer to a complete record in the main database ﬁle. The index ﬁle 341  342 Chap. 10 Indexing could be sorted or organized using a tree structure, thereby imposing a logical or- der on the records without physically rearranging them. One database might have several associated index ﬁles, each supporting efﬁcient access through a different key ﬁeld. Each record of a database normally has a unique identiﬁer, called the primary key. For example, the primary key for a set of personnel records might be the Social Security number or ID number for the individual. Unfortunately, the ID number is generally an inconvenient value on which to perform a search because the searcher is unlikely to know it. Instead, the searcher might know the desired employee’s name. Alternatively, the searcher might be interested in ﬁnding all employees whose salary is in a certain range. If these are typical search requests to the database, then the name and salary ﬁelds deserve separate indices. However, key values in the name and salary indices are not likely to be unique. A key ﬁeld such as salary, where a particular key value might be duplicated in multiple records, is called a secondary key. Most searches are performed using a secondary key. The secondary key index (or more simply, secondary index) will associate a secondary key value with the primary key of each record having that secondary key value. At this point, the full database might be searched directly for the record with that primary key, or there might be a primary key index (or primary index) that relates each primary key value with a pointer to the actual record on disk. In the latter case, only the primary index provides the location of the actual record on disk, while the secondary indices refer to the primary index. Indexing is an important technique for organizing large databases, and many indexing methods have been developed. Direct access through hashing is discussed in Section 9.4. A simple list sorted by key value can also serve as an index to the record ﬁle. Indexing disk ﬁles by sorted lists are discussed in the following section. Unfortunately, a sorted list does not perform well for insert and delete operations. A third approach to indexing is the tree index.Under extreme conditions, many searches might eventually lead to the system overﬂow area. The “solution” to this problem is to periodically reorganize the entire database. This means re-balancing the records 1Recall from Section 8.2.1 that a cylinder is all of the tracks readable from a particular placement of the heads on the multiple platters of a disk drive. 348 Chap. 10 Indexing among the cylinders, sorting the records within each cylinder, and updating both the system index table and the within-cylinder block table. Such reorganization was typical of database systems during the 1960s and would normally be done each night or weekly. 10.3 Tree-based Indexing Linear indexing is efﬁcient when the database is static, that is, when records are inserted and deleted rarely or never. ISAM is adequate for a limited number of updates, but not for frequent changes. Because it has essentially two levels of indexing, ISAM will also break down for a truly large database where the number of cylinders is too great for the top-level index to ﬁt in main memory. In their most general form, database applications have the following character- istics: 1.Trees are typically used to or- ganize large databases that must support record insertion, deletion, and key range searches. Section 10.2 brieﬂy describes ISAM, a tentative step toward solving the problem of storing a large database that must support insertion and deletion of records. Its shortcomings help to illustrate the value of tree indexing techniques.4..\n",
      "  - actual output: Here are the five most important learning concepts for the chapter on Indexing: 1. **Indexing Fundamentals**: Understanding the basic concept of indexing, which involves associating a key with the location of a corresponding data record. This is crucial for enabling efficient search operations, especially in large databases where multiple search keys are involved. 2. **Primary and Secondary Keys**: Differentiating between primary keys (unique identifiers for records) and secondary keys (non-unique fields that may be used for searching). Recognizing the role of primary and secondary indices in facilitating various types of queries is essential for effective database management. 3. **Types of Index Structures**: Familiarity with different indexing methods, including sorted lists, hash tables, and tree-based structures (like B-trees). Each method has its strengths and weaknesses, particularly concerning insertion, deletion, and search operations, which are critical for optimizing database performance. 4. **Handling Multiple Search Keys**: Understanding the need for multiple indices to support different search keys in a database. This includes the creation of separate indices for fields like names and salaries, which may not be unique but are commonly used in search queries. 5. **Reorganization and Maintenance of Indexes**: Recognizing the importance of periodically reorganizing and maintaining indexes to ensure efficient performance, especially in dynamic databases where records are frequently inserted or deleted. This includes understanding techniques like rebalancing and the implications of database growth on indexing strategies.  These concepts provide a foundational understanding of how indexing works in databases and the various strategies employed to optimize data retrieval and management.\n",
      "  - expected output: Data Structures Basics Algorithm Analysis O Notation Algorithms Non-recursive Algorthims Non-recursive Algorithms Search Linear Search Binary Search Sort Insertion Sort Selection Sort Bubble Sort Recursive Algorithms Recursive Binary Search Recursive Sort Merge Sort Quick Sort\n",
      "  - context: None\n",
      "  - retrieval context: ['The classic example is a large database of records with multiple search keys, requiring the ability to insert, delete, and search for records. Hashing provides outstanding performance for such situations, but only in the limited case in which all searches are of the form “ﬁnd the record with key value K.” Many applications require more general search capabilities. One exam- ple is a range query search for all records whose key lies within some range. Other queries might involve visiting all records in order of their key value, or ﬁnding the record with the greatest key value. Hash tables are not organized to support any of these queries efﬁciently. This chapter introduces ﬁle structures used to organize a large collection of records stored on disk. Such ﬁle structures support efﬁcient insertion, deletion, and search operations, for exact-match queries, range queries, and largest/smallest key value searches. Before discussing such ﬁle structures, we must become familiar with some ba- sic ﬁle-processing terminology. An entry-sequenced ﬁle stores records in the order that they were added to the ﬁle. Entry-sequenced ﬁles are the disk-based equivalent to an unsorted list and so do not support efﬁcient search. The natural solution is to sort the records by order of the search key. However, a typical database, such as a collection of employee or customer records maintained by a business, might con- tain multiple search keys. To answer a question about a particular customer might require a search on the name of the customer. Businesses often wish to sort and output the records by zip code order for a bulk mailing. Government paperwork might require the ability to search by Social Security number. Thus, there might not be a single “correct” order in which to store the records. Indexing is the process of associating a key with the location of a correspond- ing data record. Section 8.5 discussed the concept of a key sort, in which an index ﬁle is created whose records consist of key/pointer pairs. Here, each key is asso- ciated with a pointer to a complete record in the main database ﬁle. The index ﬁle 341  342 Chap. 10 Indexing could be sorted or organized using a tree structure, thereby imposing a logical or- der on the records without physically rearranging them. One database might have several associated index ﬁles, each supporting efﬁcient access through a different key ﬁeld. Each record of a database normally has a unique identiﬁer, called the primary key. For example, the primary key for a set of personnel records might be the Social Security number or ID number for the individual. Unfortunately, the ID number is generally an inconvenient value on which to perform a search because the searcher is unlikely to know it. Instead, the searcher might know the desired employee’s name. Alternatively, the searcher might be interested in ﬁnding all employees whose salary is in a certain range. If these are typical search requests to the database, then the name and salary ﬁelds deserve separate indices. However, key values in the name and salary indices are not likely to be unique. A key ﬁeld such as salary, where a particular key value might be duplicated in multiple records, is called a secondary key. Most searches are performed using a secondary key. The secondary key index (or more simply, secondary index) will associate a secondary key value with the primary key of each record having that secondary key value. At this point, the full database might be searched directly for the record with that primary key, or there might be a primary key index (or primary index) that relates each primary key value with a pointer to the actual record on disk. In the latter case, only the primary index provides the location of the actual record on disk, while the secondary indices refer to the primary index. Indexing is an important technique for organizing large databases, and many indexing methods have been developed. Direct access through hashing is discussed in Section 9.4. A simple list sorted by key value can also serve as an index to the record ﬁle. Indexing disk ﬁles by sorted lists are discussed in the following section. Unfortunately, a sorted list does not perform well for insert and delete operations. A third approach to indexing is the tree index.Under extreme conditions, many searches might eventually lead to the system overﬂow area. The “solution” to this problem is to periodically reorganize the entire database. This means re-balancing the records 1Recall from Section 8.2.1 that a cylinder is all of the tracks readable from a particular placement of the heads on the multiple platters of a disk drive. 348 Chap. 10 Indexing among the cylinders, sorting the records within each cylinder, and updating both the system index table and the within-cylinder block table. Such reorganization was typical of database systems during the 1960s and would normally be done each night or weekly. 10.3 Tree-based Indexing Linear indexing is efﬁcient when the database is static, that is, when records are inserted and deleted rarely or never. ISAM is adequate for a limited number of updates, but not for frequent changes. Because it has essentially two levels of indexing, ISAM will also break down for a truly large database where the number of cylinders is too great for the top-level index to ﬁt in main memory. In their most general form, database applications have the following character- istics: 1.Trees are typically used to or- ganize large databases that must support record insertion, deletion, and key range searches. Section 10.2 brieﬂy describes ISAM, a tentative step toward solving the problem of storing a large database that must support insertion and deletion of records. Its shortcomings help to illustrate the value of tree indexing techniques.4.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Contextual Precision: 50.00% pass rate\n",
      "Contextual Recall: 25.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric, ContextualPrecisionMetric, ContextualRecallMetric, FaithfulnessMetric\n",
    "metrics = [AnswerRelevancyMetric(), ContextualPrecisionMetric(), ContextualRecallMetric(), FaithfulnessMetric()]\n",
    "samples = extractor.evaluate('concepts', 5, concepts, actual_concepts, retrieved, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Precision Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Precision Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Recall Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Recall Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-4o, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-4o, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating 4 test case(s) in parallel: |██████████|100% (4/4) [Time Taken: 00:24,  6.23s/test case]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the response perfectly addresses the input without any irrelevant statements. Great job!, error: None)\n",
      "  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the relevant nodes are ranked appropriately, discussing key concepts like 'running time', 'comparisons', and the '80/20 rule' which align perfectly with the expected learning concepts for the chapter on Searching. Great job on the precision!, error: None)\n",
      "  - ❌ Contextual Recall (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the expected output, which is a list of topics and terms, does not correspond to any nodes in the retrieval context., error: None)\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions. Great job maintaining consistency!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Identify the 5 most important learning concepts for chapter Searching. The relevant context can be found here: care right precise value c might . concerned time required increment vari able must done value array , time actual assignment larger value found , little bit extra time taken initialize currlarge . want reasonable ap proximation time taken execute algorithm . total time run largest therefore approximately cn , must make n comparison , comparison costing c time . say function largest extension , largestvalue sequential search algorithm typical implementation ha running time expressed equa tion tn = cn . equation describes growth rate running time largest value sequential search algorithm . example 3.2 running time statement assigns ﬁrst value integer array variable simply time required copy value ﬁrst array value . assume assignment take constant amount time regardless value . let u call c1 amount time necessary copy integer . matter large array typical computer given reasonable condition memory array size , time copy value ﬁrst position array always c1 . thus , equation algorithm simply tn = c1 , indicating size input n ha effect running time . called constant running time . example 3.3 consider following code : sum = 0 ; i=1 ; < =n ; i++ j=1 ; j < =n ; j++ sum++ ; running time code fragment ? clearly take longer run n larger . basic operation example increment sec . 3.1 introduction 57 0 100 200 300 400 10n 20n 2n2 5n log n 2n n ! sequential search unsorted list requires θn time worst case . many comparison doe linear search average ? example , expected number access constant . probability accessing ﬁrst record high one half , second much lower one quarter still much higher third record , . show probability distribution , dering list frequency yield efﬁcient search technique . many search application , real access pattern follow rule thumb called 80/20 rule . 80/20 rule say 80 record access 20 record . value 80 20 estimate ; every data access pat tern ha value . however , behavior nature occurs surprisingly often practice explains success caching technique widely used web browser speeding access web page , disk drive cpu manufac turers speeding access data stored slower memory ; see discussion buffer pool section 8.3. 80/20 rule applies , expect consid erable improvement search performance list ordered frequency access standard sequential search unordered list . example 9.3 80/20 rule example zipf distribution . nat urally occurring distribution often follow zipf distribution . example include observed frequency use word natural language english , size population city i.e. , view relative proportion population equivalent “ frequency use ” . zipf distribution related harmonic series deﬁned equa tion 2.10.the classic example large database record multiple search key , requiring ability insert , delete , search record . hashing provides outstanding performance situation , limited case search form “ ﬁnd record key value k. ” many application require general search capability . one exam ple range query search record whose key lie within range . query might involve visiting record order key value , ﬁnding record greatest key value . hash table organized support query efﬁciently . chapter introduces ﬁle structure used organize large collection record stored disk . ﬁle structure support efﬁcient insertion , deletion , search operation , exactmatch query , range query , largest/smallest key value search . discussing ﬁle structure , must become familiar ba sic ﬁleprocessing terminology . entrysequenced ﬁle store record order added ﬁle . entrysequenced ﬁles diskbased equivalent unsorted list support efﬁcient search . natural solution sort record order search key . however , typical database , collection employee customer record maintained business , might con tain multiple search key . answer question particular customer might require search name customer . business often wish sort output record zip code order bulk mailing . government paperwork might require ability search social security number . thus , might single “ correct ” order store record . indexing process associating key location correspond ing data record . section 8.5 discussed concept key sort , index ﬁle created whose record consist key/pointer pair . , key asso ciated pointer complete record main database ﬁle . index ﬁle 341 342 chap . 10 indexing could sorted organized using tree structure , thereby imposing logical der record without physically rearranging . one database might several associated index ﬁles , supporting efﬁcient access different key ﬁeld . record database normally ha unique identiﬁer , called primary key . example , primary key set personnel record might social security number id number individual . unfortunately , id number generally inconvenient value perform search searcher unlikely know . instead , searcher might know desired employee ’ name . alternatively , searcher might interested ﬁnding employee whose salary certain range . typical search request database , name salary ﬁelds deserve separate index . however , key value name salary index likely unique . key ﬁeld salary , particular key value might duplicated multiple record , called secondary key . search performed using secondary key . secondary key index simply , secondary index associate secondary key value primary key record secondary key value . point , full database might searched directly record primary key , might primary key index primary index relates primary key value pointer actual record disk . latter case , primary index provides location actual record disk , secondary index refer primary index . indexing important technique organizing large database , many indexing method developed . direct access hashing discussed section 9.4. simple list sorted key value also serve index record ﬁle . indexing disk ﬁles sorted list discussed following section . unfortunately , sorted list doe perform well insert delete operation . third approach indexing tree index ..\n",
      "  - actual output: based provided context regarding chapter searching , five important learning concept : 1. time complexity search algorithm : understanding time complexity different search algorithm crucial . example , sequential search algorithm ha running time tn = cn , indicating time taken grows linearly number element n array . concept help evaluating efficiency search algorithm . 2. constant vs. linear time : distinction constant time tn = c1 linear time tn = cn important . constant time indicates execution time doe depend size input , linear time indicates execution time increase proportionally input size . understanding fundamental analyzing algorithm performance . 3. 80/20 rule access pattern : 80/20 rule , state 80 record access 20 record , highlight importance understanding access pattern search operation . concept lead optimization , ordering data frequency access improve search performance . 4. indexing key structure : indexing critical technique organizing large database facilitate efficient search operation . understanding difference primary secondary key , well index structured e.g. , sorted list , tree index , essential optimizing search query database . 5. hashing search capability : hashing provides method efficient searching , particularly exactmatch query . however , important recognize limitation complex query , range query . understanding use hashing versus indexing method key effective database management search optimization . concept form foundation understanding searching algorithm application data structure database .\n",
      "  - expected output: data structure basic algorithm analysis notation algorithm nonrecursive algorthims nonrecursive algorithm search linear search binary search sort insertion sort selection sort bubble sort recursive algorithm recursive binary search recursive sort merge sort quick sort\n",
      "  - context: None\n",
      "  - retrieval context: ['care right precise value c might . concerned time required increment vari able must done value array , time actual assignment larger value found , little bit extra time taken initialize currlarge . want reasonable ap proximation time taken execute algorithm . total time run largest therefore approximately cn , must make n comparison , comparison costing c time . say function largest extension , largestvalue sequential search algorithm typical implementation ha running time expressed equa tion tn = cn . equation describes growth rate running time largest value sequential search algorithm . example 3.2 running time statement assigns ﬁrst value integer array variable simply time required copy value ﬁrst array value . assume assignment take constant amount time regardless value . let u call c1 amount time necessary copy integer . matter large array typical computer given reasonable condition memory array size , time copy value ﬁrst position array always c1 . thus , equation algorithm simply tn = c1 , indicating size input n ha effect running time . called constant running time . example 3.3 consider following code : sum = 0 ; i=1 ; < =n ; i++ j=1 ; j < =n ; j++ sum++ ; running time code fragment ? clearly take longer run n larger . basic operation example increment sec . 3.1 introduction 57 0 100 200 300 400 10n 20n 2n2 5n log n 2n n ! sequential search unsorted list requires θn time worst case . many comparison doe linear search average ? example , expected number access constant . probability accessing ﬁrst record high one half , second much lower one quarter still much higher third record , . show probability distribution , dering list frequency yield efﬁcient search technique . many search application , real access pattern follow rule thumb called 80/20 rule . 80/20 rule say 80 record access 20 record . value 80 20 estimate ; every data access pat tern ha value . however , behavior nature occurs surprisingly often practice explains success caching technique widely used web browser speeding access web page , disk drive cpu manufac turers speeding access data stored slower memory ; see discussion buffer pool section 8.3. 80/20 rule applies , expect consid erable improvement search performance list ordered frequency access standard sequential search unordered list . example 9.3 80/20 rule example zipf distribution . nat urally occurring distribution often follow zipf distribution . example include observed frequency use word natural language english , size population city i.e. , view relative proportion population equivalent “ frequency use ” . zipf distribution related harmonic series deﬁned equa tion 2.10.the classic example large database record multiple search key , requiring ability insert , delete , search record . hashing provides outstanding performance situation , limited case search form “ ﬁnd record key value k. ” many application require general search capability . one exam ple range query search record whose key lie within range . query might involve visiting record order key value , ﬁnding record greatest key value . hash table organized support query efﬁciently . chapter introduces ﬁle structure used organize large collection record stored disk . ﬁle structure support efﬁcient insertion , deletion , search operation , exactmatch query , range query , largest/smallest key value search . discussing ﬁle structure , must become familiar ba sic ﬁleprocessing terminology . entrysequenced ﬁle store record order added ﬁle . entrysequenced ﬁles diskbased equivalent unsorted list support efﬁcient search . natural solution sort record order search key . however , typical database , collection employee customer record maintained business , might con tain multiple search key . answer question particular customer might require search name customer . business often wish sort output record zip code order bulk mailing . government paperwork might require ability search social security number . thus , might single “ correct ” order store record . indexing process associating key location correspond ing data record . section 8.5 discussed concept key sort , index ﬁle created whose record consist key/pointer pair . , key asso ciated pointer complete record main database ﬁle . index ﬁle 341 342 chap . 10 indexing could sorted organized using tree structure , thereby imposing logical der record without physically rearranging . one database might several associated index ﬁles , supporting efﬁcient access different key ﬁeld . record database normally ha unique identiﬁer , called primary key . example , primary key set personnel record might social security number id number individual . unfortunately , id number generally inconvenient value perform search searcher unlikely know . instead , searcher might know desired employee ’ name . alternatively , searcher might interested ﬁnding employee whose salary certain range . typical search request database , name salary ﬁelds deserve separate index . however , key value name salary index likely unique . key ﬁeld salary , particular key value might duplicated multiple record , called secondary key . search performed using secondary key . secondary key index simply , secondary index associate secondary key value primary key record secondary key value . point , full database might searched directly record primary key , might primary key index primary index relates primary key value pointer actual record disk . latter case , primary index provides location actual record disk , secondary index refer primary index . indexing important technique organizing large database , many indexing method developed . direct access hashing discussed section 9.4. simple list sorted key value also serve index record ﬁle . indexing disk ﬁles sorted list discussed following section . unfortunately , sorted list doe perform well insert delete operation . third approach indexing tree index .']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output directly addresses the input with no irrelevant statements. Great job staying focused on the task!, error: None)\n",
      "  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the relevant node discussing 'quicksort' and its comparison with 'mergesort' is perfectly aligned with the input query about important learning concepts for internal sorting, and there are no irrelevant nodes affecting the ranking. Great job at ranking!, error: None)\n",
      "  - ❌ Contextual Recall (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because none of the terms in the expected output are supported by any nodes in the retrieval context, demonstrating a complete mismatch., error: None)\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: Fantastic job! The score is 1.00 because there are no contradictions between the actual output and the retrieval context. Keep up the great work!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Identify the 5 most important learning concepts for chapter Internal Sorting. The relevant context can be found here: must careful interpret last statement , however . world certainly better invention quicksort , even though mergesort wa available time . quicksort asymptotically faster mergesort , yet merely “ tuning ” mergesort either . quicksort substantially different approach sorting . even upper lower bound problem meet , still beneﬁts gained new , clever algorithm.7.5 quicksort 241 still unlikely happen . doe take many good partitioning quicksort work fairly well . quicksort ’ best case occurs findpivot always break array two equal half . quicksort repeatedly split array smaller partition , shown figure 7.14. best case , result log n level partition , top level one array size n , second level two array size n/2 , next four array size n/4 , . thus , level , partition step level total n work , overall cost n log n work quicksort ﬁnds perfect pivot . quicksort ’ averagecase behavior fall somewhere extreme worst best case . averagecase analysis considers cost possible ar rangements input , summing cost dividing number case . make one reasonable simplifying assumption : partition step , pivot equally likely end position sorted array . word , pivot equally likely break array partition size 0 n−1 , 1 n−2 , . given assumption , averagecase cost computed following equation : tn = cn + 1 n n−1 x k=0 [ tk + tn −1 −k ] , t0 = t1 = c. equation form recurrence relation.if , solve subproblem recursively consid ering one sublists . , pivot end position k > , 500 chap . 15 lower bound figure 15.5 method ﬁnding pivot partitioning list guarantee least ﬁxed fraction list partition . divide list group ﬁve element , ﬁnd median group . recursively ﬁnd median n/5 median . median ﬁve element guaran teed least two partition . median three median collection 15 element guaranteed least ﬁve element partition . simply solve ﬁnding ith best element left partition . pivot position k < , wish ﬁnd −kth element right partition . worst case cost algorithm ? quicksort , get bad performance pivot ﬁrst last element array . would lead possibly on2 performance . however , pivot always cut array half , cost would modeled recurrence tn = tn/2 +n = 2n cost . finding average cost requires u use recurrence full history , similar one used model cost quicksort . , ﬁnd tn average case . possible modify algorithm get worstcase linear time ? , need pick pivot guaranteed discard ﬁxed fraction element . choose pivot random , meet guarantee . ideal situation would could pick median value pivot time . essentially problem trying solve begin . notice , however , choose constant c , pick median sample size n/c , guarantee discard least n/2c element . actually , better selecting small subset constant size ﬁnd median constant time , taking median median . figure 15.5 illustrates idea . observation lead directly following algorithm . • choose n/5 median group ﬁve element list . choosing median ﬁve item done constant time . • recursively , select , median n/5 mediansofﬁves . • partition list element larger smaller m. sec . 15.7 optimal sorting 501 selecting median way guaranteed eliminate fraction element leaving ⌈7n −5/10⌉elements left , still need sure recursion yield lineartime algorithm . model algorithm following recurrence . tn ≤t⌈n/5⌉ + t⌈7n −5/10⌉ + 6⌈n/5⌉+ n −1 . t⌈n/5⌉ term come computing median mediansofﬁves , 6⌈n/5⌉term come cost calculate medianofﬁves exactly six comparison group ﬁve element , t⌈7n−5/10⌉ term come recursive call remaining 70 element might left . prove recurrence linear assuming true constant r , show tn ≤rn n greater bound . tn ≤ t⌈n 5 ⌉ + t⌈7n −5 10 ⌉ + 6⌈n 5 ⌉+ n −1 ≤ rn 5 + 1 + r7n −5 10 + 1 + 6n 5 + 1 + n −1 ≤ r 5 + 7r 10 + 11 5 n + 3r 2 + 5 ≤ 9r + 22 10 n + 3r + 10 2 . true r ≥23 n ≥380 . provides base case allows u use induction prove ∀n ≥380 , tn ≤23n . reality , algorithm practical constant factor cost high.some algorithm straightforward adaptation scheme use everyday life . others totally alien hu man thing , invented sort thousand even million record stored computer . year study , still unsolved problem related sorting . new algorithm still developed reﬁned special purpose application . introducing central problem computer science , chapter ha secondary purpose illustrating issue algorithm design analysis . example , collection sorting algorithm show multiple approach u ing divideandconquer . particular , multiple way dividing : mergesort divide list half ; quicksort divide list big value small value ; radix sort divide problem working one digit key time . sorting algorithm also illustrate wide variety analysis technique . ’ ﬁnd possible algorithm average case whose growth rate signiﬁcantly smaller worse case quicksort . ’ see possible speed sorting algorithm shellsort quicksort taking advantage best case behavior another algorithm insertion sort . ’ see several example tune algorithm better performance . ’ see special case behavior algorithm make good solution 223 224 chap . 7 internal sorting special niche application heapsort . sorting provides example signiﬁcant technique analyzing lower bound problem . sorting also used motivate introduction ﬁle processing presented chapter 8. present chapter cover several standard algorithm appropriate sorting collection record ﬁt computer ’ main memory . begin dis cussion three simple , relatively slow , algorithm requiring θn2 time average worst case . several algorithm considerably better performance presented , θn log n worstcase running time . ﬁnal sort ing method presented requires θn worstcase time special condition . chapter concludes proof sorting general requires ωn log n time worst case . 7.1 sorting terminology notation except noted otherwise , input sorting algorithm presented chapter collection record stored array . record compared one another requiring type extend comparable class . ensure class implement compareto method , return value less zero , equal zero , greater zero depending relationship record compared . compareto method deﬁned extract appropriate key ﬁeld record . also assume every record type swap function interchange content two record array . given set record r1 , r2 , ... , rn key value k1 , k2 , ... , kn , sorting problem arrange record order record rs1 , rs2 , ... , rsn key obeying property ks1 ≤ks2 ≤ ... ≤ksn . word , sorting problem arrange set record value key ﬁelds nondecreasing order . deﬁned , sorting problem allows input two record key value . certain application require input contain duplicate key value . sorting algorithm presented chapter chapter 8 handle duplicate key value unless noted otherwise . duplicate key value allowed , might implicit ordering duplicate , typically based order occurrence within input . might desirable maintain initial ordering among duplicate . sorting algorithm said stable doe change relative ordering record identical key value . many , , sorting algorithm presented chapter stable , made stable minor change . comparing two sorting algorithm , straightforward approach would seem simply program measure running time . ex ample timing presented figure 7.20. however , comparison sec . 7.2 three θn2 sorting algorithm 225 misleading running time many sorting algorithm depends speciﬁcs input value . particular , number record , size key record , allowable range key value , amount input record “ order ” greatly affect relative running time sorting algorithm . analyzing sorting algorithm , traditional measure number comparison made key . measure usually closely related running time algorithm ha advantage machine data type independent . however , case record might large physical movement might take signiﬁcant fraction total running time . , might appropriate measure number swap operation performed algorithm . application assume record key ﬁxed length , single comparison single swap operation requires constant amount time regardless key involved . special situation “ change rule ” comparing sorting algorithm . example , application record key widely varying length sorting sequence variable length string beneﬁt specialpurpose sorting technique . application require small number record sorted , sort performed frequently . example would application repeatedly sort group ﬁve number . case , constant runtime equation usually ignored asymptotic analysis become crucial . finally , situation require sorting algorithm use little memory possible . note sorting algorithm require signiﬁcant extra memory beyond input array . 7.2 three θn2 sorting algorithm section present three simple sorting algorithm . easy understand implement , soon see unacceptably slow many record sort ..\n",
      "  - actual output: five important learning concept chapter internal sorting , particularly focusing discussion sorting algorithm like quicksort mergesort : 1. algorithm efficiency complexity : understanding time complexity sorting algorithm crucial . chapter discusses various sorting algorithm , highlighting average , best , worstcase performance . instance , quicksort ha averagecase time complexity log n degrade on² worst case poor pivot choice made . concept emphasizes importance analyzing comparing efficiency different algorithm . 2. divideandconquer strategy : many sorting algorithm , including quicksort mergesort , utilize divideandconquer approach . involves breaking problem smaller subproblems , solving subproblem independently , combining result . understanding strategy essential grasping algorithm operate underlying principle . 3. pivot selection quicksort : choice pivot quicksort significantly affect performance . bestcase scenario occurs pivot divide array two equal half , leading optimal performance . chapter also discusses strategy improving pivot selection avoid worstcase scenario , using median median ensure balanced partitioning . 4. stability sorting algorithm : concept stability sorting algorithm introduced , stable sorting algorithm maintains relative order record equal key . important application order record matter . chapter note algorithm inherently stable , others modified achieve stability . 5. lower bound sorting : chapter concludes discussion theoretical lower bound sorting algorithm , ωn log n worst case . concept fundamental understanding limitation sorting algorithm inherent complexity sorting problem , regardless algorithm used . concept provide foundational understanding sorting algorithm , analysis , practical implication computer science .\n",
      "  - expected output: data structure basic algorithm analysis notation algorithm nonrecursive algorthims nonrecursive algorithm search linear search binary search sort insertion sort selection sort bubble sort recursive algorithm recursive binary search recursive sort merge sort quick sort\n",
      "  - context: None\n",
      "  - retrieval context: ['must careful interpret last statement , however . world certainly better invention quicksort , even though mergesort wa available time . quicksort asymptotically faster mergesort , yet merely “ tuning ” mergesort either . quicksort substantially different approach sorting . even upper lower bound problem meet , still beneﬁts gained new , clever algorithm.7.5 quicksort 241 still unlikely happen . doe take many good partitioning quicksort work fairly well . quicksort ’ best case occurs findpivot always break array two equal half . quicksort repeatedly split array smaller partition , shown figure 7.14. best case , result log n level partition , top level one array size n , second level two array size n/2 , next four array size n/4 , . thus , level , partition step level total n work , overall cost n log n work quicksort ﬁnds perfect pivot . quicksort ’ averagecase behavior fall somewhere extreme worst best case . averagecase analysis considers cost possible ar rangements input , summing cost dividing number case . make one reasonable simplifying assumption : partition step , pivot equally likely end position sorted array . word , pivot equally likely break array partition size 0 n−1 , 1 n−2 , . given assumption , averagecase cost computed following equation : tn = cn + 1 n n−1 x k=0 [ tk + tn −1 −k ] , t0 = t1 = c. equation form recurrence relation.if , solve subproblem recursively consid ering one sublists . , pivot end position k > , 500 chap . 15 lower bound figure 15.5 method ﬁnding pivot partitioning list guarantee least ﬁxed fraction list partition . divide list group ﬁve element , ﬁnd median group . recursively ﬁnd median n/5 median . median ﬁve element guaran teed least two partition . median three median collection 15 element guaranteed least ﬁve element partition . simply solve ﬁnding ith best element left partition . pivot position k < , wish ﬁnd −kth element right partition . worst case cost algorithm ? quicksort , get bad performance pivot ﬁrst last element array . would lead possibly on2 performance . however , pivot always cut array half , cost would modeled recurrence tn = tn/2 +n = 2n cost . finding average cost requires u use recurrence full history , similar one used model cost quicksort . , ﬁnd tn average case . possible modify algorithm get worstcase linear time ? , need pick pivot guaranteed discard ﬁxed fraction element . choose pivot random , meet guarantee . ideal situation would could pick median value pivot time . essentially problem trying solve begin . notice , however , choose constant c , pick median sample size n/c , guarantee discard least n/2c element . actually , better selecting small subset constant size ﬁnd median constant time , taking median median . figure 15.5 illustrates idea . observation lead directly following algorithm . • choose n/5 median group ﬁve element list . choosing median ﬁve item done constant time . • recursively , select , median n/5 mediansofﬁves . • partition list element larger smaller m. sec . 15.7 optimal sorting 501 selecting median way guaranteed eliminate fraction element leaving ⌈7n −5/10⌉elements left , still need sure recursion yield lineartime algorithm . model algorithm following recurrence . tn ≤t⌈n/5⌉ + t⌈7n −5/10⌉ + 6⌈n/5⌉+ n −1 . t⌈n/5⌉ term come computing median mediansofﬁves , 6⌈n/5⌉term come cost calculate medianofﬁves exactly six comparison group ﬁve element , t⌈7n−5/10⌉ term come recursive call remaining 70 element might left . prove recurrence linear assuming true constant r , show tn ≤rn n greater bound . tn ≤ t⌈n 5 ⌉ + t⌈7n −5 10 ⌉ + 6⌈n 5 ⌉+ n −1 ≤ rn 5 + 1 + r7n −5 10 + 1 + 6n 5 + 1 + n −1 ≤ r 5 + 7r 10 + 11 5 n + 3r 2 + 5 ≤ 9r + 22 10 n + 3r + 10 2 . true r ≥23 n ≥380 . provides base case allows u use induction prove ∀n ≥380 , tn ≤23n . reality , algorithm practical constant factor cost high.some algorithm straightforward adaptation scheme use everyday life . others totally alien hu man thing , invented sort thousand even million record stored computer . year study , still unsolved problem related sorting . new algorithm still developed reﬁned special purpose application . introducing central problem computer science , chapter ha secondary purpose illustrating issue algorithm design analysis . example , collection sorting algorithm show multiple approach u ing divideandconquer . particular , multiple way dividing : mergesort divide list half ; quicksort divide list big value small value ; radix sort divide problem working one digit key time . sorting algorithm also illustrate wide variety analysis technique . ’ ﬁnd possible algorithm average case whose growth rate signiﬁcantly smaller worse case quicksort . ’ see possible speed sorting algorithm shellsort quicksort taking advantage best case behavior another algorithm insertion sort . ’ see several example tune algorithm better performance . ’ see special case behavior algorithm make good solution 223 224 chap . 7 internal sorting special niche application heapsort . sorting provides example signiﬁcant technique analyzing lower bound problem . sorting also used motivate introduction ﬁle processing presented chapter 8. present chapter cover several standard algorithm appropriate sorting collection record ﬁt computer ’ main memory . begin dis cussion three simple , relatively slow , algorithm requiring θn2 time average worst case . several algorithm considerably better performance presented , θn log n worstcase running time . ﬁnal sort ing method presented requires θn worstcase time special condition . chapter concludes proof sorting general requires ωn log n time worst case . 7.1 sorting terminology notation except noted otherwise , input sorting algorithm presented chapter collection record stored array . record compared one another requiring type extend comparable class . ensure class implement compareto method , return value less zero , equal zero , greater zero depending relationship record compared . compareto method deﬁned extract appropriate key ﬁeld record . also assume every record type swap function interchange content two record array . given set record r1 , r2 , ... , rn key value k1 , k2 , ... , kn , sorting problem arrange record order record rs1 , rs2 , ... , rsn key obeying property ks1 ≤ks2 ≤ ... ≤ksn . word , sorting problem arrange set record value key ﬁelds nondecreasing order . deﬁned , sorting problem allows input two record key value . certain application require input contain duplicate key value . sorting algorithm presented chapter chapter 8 handle duplicate key value unless noted otherwise . duplicate key value allowed , might implicit ordering duplicate , typically based order occurrence within input . might desirable maintain initial ordering among duplicate . sorting algorithm said stable doe change relative ordering record identical key value . many , , sorting algorithm presented chapter stable , made stable minor change . comparing two sorting algorithm , straightforward approach would seem simply program measure running time . ex ample timing presented figure 7.20. however , comparison sec . 7.2 three θn2 sorting algorithm 225 misleading running time many sorting algorithm depends speciﬁcs input value . particular , number record , size key record , allowable range key value , amount input record “ order ” greatly affect relative running time sorting algorithm . analyzing sorting algorithm , traditional measure number comparison made key . measure usually closely related running time algorithm ha advantage machine data type independent . however , case record might large physical movement might take signiﬁcant fraction total running time . , might appropriate measure number swap operation performed algorithm . application assume record key ﬁxed length , single comparison single swap operation requires constant amount time regardless key involved . special situation “ change rule ” comparing sorting algorithm . example , application record key widely varying length sorting sequence variable length string beneﬁt specialpurpose sorting technique . application require small number record sorted , sort performed frequently . example would application repeatedly sort group ﬁve number . case , constant runtime equation usually ignored asymptotic analysis become crucial . finally , situation require sorting algorithm use little memory possible . note sorting algorithm require signiﬁcant extra memory beyond input array . 7.2 three θn2 sorting algorithm section present three simple sorting algorithm . easy understand implement , soon see unacceptably slow many record sort .']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output is perfectly relevant, providing a focused and precise response without any irrelevant information. Great job!, error: None)\n",
      "  - ✅ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because all the relevant nodes are perfectly ranked, with no irrelevant nodes present. Excellent job keeping everything in line!, error: None)\n",
      "  - ✅ Contextual Recall (score: 0.8888888888888888, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.89 because most of the expected outputs are well-supported by the nodes in the retrieval context, specifically node 2, which covers a wide range of sorting algorithms and search methods. However, the first sentence of the expected output is a broad list of topics that does not directly match any specific part of the retrieval context, leading to a slight decrease in score., error: None)\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions; the actual output perfectly aligns with the retrieval context. Great job!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Identify the 5 most important learning concepts for chapter File Processing and External Sorting. The relevant context can be found here: 234 chap . 7 internal sorting 36 20 17 13 28 14 23 15 28 23 15 14 36 20 17 13 20 36 13 17 14 28 15 23 13 14 15 17 20 23 28 36 figure 7.8 illustration mergesort . ﬁrst row show eight number sorted . mergesort recursively subdivide list sublists one element , recombine sublists . second row show four sublists size 2 created ﬁrst merging pas . third row show two sublists size 4 created next merging pas sublists row 2. last row show ﬁnal sorted list created merging two sublists row 3. mergesort one simplest sorting algorithm conceptually , ha good performance asymptotic sense empirical running time . surpris ingly , even though based simple concept , relatively difﬁcult im plement practice . figure 7.8 illustrates mergesort . pseudocode sketch mergesort follows : list mergesortlist inlist { inlist.length < = 1 return inlist ; ; list l1 = half item inlist ; list l2 = half item inlist ; return mergemergesortl1 , mergesortl2 ; } discussing implement mergesort , ﬁrst examine merge function . merging two sorted sublists quite simple . function merge examines ﬁrst element sublist pick smaller value smallest element overall . smaller value removed sublist placed output list . merging continues way , comparing front element sublists continually appending smaller output list input element remain . implementing mergesort present number technical difﬁculties . ﬁrst decision represent list . mergesort lends well sorting singly linked list merging doe require random access list element . thus , mergesort method choice input form linked list . implementing merge linked list straightforward , need remove item front input list append item output list . breaking input list two equal half present difﬁculty.these assumption 284 chap . 8 file processing external sorting relaxed specialpurpose sorting application , ignoring complication make principle clearer . explained section 8.2 , sector basic unit i/o . word , disk read writes one complete sector . sector size typically power two , range 512 16k byte , depending operating system size speed disk drive . block size used external sorting algorithm equal multiple sector size . model , sorting algorithm read block data buffer main memory , performs processing , future time writes back disk . section 8.1 see reading writing block disk take order one million time longer memory access . based fact , reasonably expect record contained single block sorted internal sorting algorithm quicksort less time required read write block . good condition , reading ﬁle sequential order efﬁcient reading block random order . given signiﬁcant impact seek time disk access , might seem obvious sequential processing faster . however , important understand precisely circumstance sequential ﬁle processing actually faster random access , affect approach designing external sorting algorithm . efﬁcient sequential access relies seek time kept minimum . ﬁrst requirement block making ﬁle fact stored disk sequential order close together , preferably ﬁlling small number contiguous track . least , number extent making ﬁle small . user typically much control layout ﬁle disk , writing ﬁle sequential order disk drive high percentage free space increase likelihood arrangement . second requirement disk drive ’ i/o head remain positioned ﬁle throughout sequential processing . happen competition kind i/o head . example , multiuser timeshared computer sorting process might compete i/o head process user . even sorting process ha sole control i/o head , still likely sequential processing efﬁcient . imagine situation processing done single disk drive , typical arrangement single bank read/write head move together stack platter . sorting process involves reading input ﬁle , alternated writing output ﬁle , i/o head continuously seek input ﬁle output ﬁle . similarly , two input ﬁles processed simultaneously merge process , i/o head continuously seek two ﬁles . sec . 8.5 external sorting 285 moral , single disk drive , often thing efﬁ cient sequential processing data ﬁle . thus , sorting algorithm might efﬁcient performs smaller number nonsequential disk operation rather larger number logically sequential disk operation require large number seek practice . mentioned previously , record size might quite large compared size key.trees typically used ganize large database must support record insertion , deletion , key range search . section 10.2 brieﬂy describes isam , tentative step toward solving problem storing large database must support insertion deletion record . shortcoming help illustrate value tree indexing techniques.274 chap . 8 file processing external sorting example illustrates important keep disk ﬁles coming fragmented , socalled “ disk defragmenters ” speed ﬁle processing time . file fragmentation happens commonly disk nearly full ﬁle manager must search free space whenever ﬁle created changed . 8.3 buﬀers buﬀer pool given speciﬁcations disk drive example 8.1 , ﬁnd take 9.5+11.1×1.5 = 26.2 read one track data average . take 9.5+11.1/2+1/256×11.1 = 15.1 average read single sector data . good saving slightly half time , less 1 data track read . want read single byte , would save u effectively time required read entire sector . reason , nearly disk drive automatically read write entire sector ’ worth information whenever disk accessed , even one byte information requested . sector read , information stored main memory . known buffering caching information . next disk request sector , necessary read disk information already stored main memory . buffering example one method minimizing disk access mentioned beginning chapter : bring additional information disk satisfy future request . information ﬁles accessed random , chance two consecutive disk request sector would low . however , practice disk request close location logical ﬁle least previous request . mean probability next request “ hitting cache ” much higher chance would indicate . principle explains one reason average access time new disk drive lower past . hardware faster , information also stored using better algorithm larger cache minimize number time information need fetched disk . concept also used store part program faster memory within cpu , using cpu cache prevalent modern microprocessor . sectorlevel buffering normally provided operating system ten built directly disk drive controller hardware . operating system maintain least two buffer , one input one output . consider would happen one buffer bytebybyte copy operation . sector containing ﬁrst byte would read i/o buffer . output operation would need destroy content single i/o buffer write byte . buffer would need ﬁlled disk second byte , sec . 8.3 buﬀers buﬀer pool 275 destroyed output . simple solution problem keep one buffer input , second output . disk drive controller operate independently cpu i/o request received . useful cpu typically execute million instruction time required single i/o operation ..\n",
      "  - actual output: five important learning concept chapter file processing external sorting : 1. understanding external sorting : external sorting crucial handling large datasets fit main memory . involves reading data disk , processing block , writing sorted data back disk . key algorithm , mergesort , adapted external sorting minimize disk i/o operation , significantly slower memory access . 2. mergesort algorithm : mergesort fundamental sorting algorithm work recursively dividing list smaller sublists , sorting sublists , merging back together . understanding implementation , especially merge function , essential grasping sorting efficiently performed large datasets . 3. disk i/o buffering : chapter emphasizes importance disk i/o operation concept buffering . since reading writing data from/to disk much slower accessing data memory , effective buffering strategy necessary minimize number disk access . includes understanding manage buffer input output operation . 4. sequential vs. random access : efficiency file processing heavily influenced access pattern data . sequential access generally faster random access due reduced seek time . understanding condition sequential processing efficient critical designing effective external sorting algorithm . 5. file fragmentation performance : file fragmentation significantly impact performance file processing . chapter discusses fragmented file lead increased seek time slower access speed . technique disk defragmentation important maintaining optimal file performance , especially context external sorting . concept provide foundational understanding file processing external sorting work , particularly context managing large datasets efficiently .\n",
      "  - expected output: data structure basic algorithm analysis notation algorithm nonrecursive algorthims nonrecursive algorithm search linear search binary search sort insertion sort selection sort bubble sort recursive algorithm recursive binary search recursive sort merge sort quick sort\n",
      "  - context: None\n",
      "  - retrieval context: ['234 chap . 7 internal sorting 36 20 17 13 28 14 23 15 28 23 15 14 36 20 17 13 20 36 13 17 14 28 15 23 13 14 15 17 20 23 28 36 figure 7.8 illustration mergesort . ﬁrst row show eight number sorted . mergesort recursively subdivide list sublists one element , recombine sublists . second row show four sublists size 2 created ﬁrst merging pas . third row show two sublists size 4 created next merging pas sublists row 2. last row show ﬁnal sorted list created merging two sublists row 3. mergesort one simplest sorting algorithm conceptually , ha good performance asymptotic sense empirical running time . surpris ingly , even though based simple concept , relatively difﬁcult im plement practice . figure 7.8 illustrates mergesort . pseudocode sketch mergesort follows : list mergesortlist inlist { inlist.length < = 1 return inlist ; ; list l1 = half item inlist ; list l2 = half item inlist ; return mergemergesortl1 , mergesortl2 ; } discussing implement mergesort , ﬁrst examine merge function . merging two sorted sublists quite simple . function merge examines ﬁrst element sublist pick smaller value smallest element overall . smaller value removed sublist placed output list . merging continues way , comparing front element sublists continually appending smaller output list input element remain . implementing mergesort present number technical difﬁculties . ﬁrst decision represent list . mergesort lends well sorting singly linked list merging doe require random access list element . thus , mergesort method choice input form linked list . implementing merge linked list straightforward , need remove item front input list append item output list . breaking input list two equal half present difﬁculty.these assumption 284 chap . 8 file processing external sorting relaxed specialpurpose sorting application , ignoring complication make principle clearer . explained section 8.2 , sector basic unit i/o . word , disk read writes one complete sector . sector size typically power two , range 512 16k byte , depending operating system size speed disk drive . block size used external sorting algorithm equal multiple sector size . model , sorting algorithm read block data buffer main memory , performs processing , future time writes back disk . section 8.1 see reading writing block disk take order one million time longer memory access . based fact , reasonably expect record contained single block sorted internal sorting algorithm quicksort less time required read write block . good condition , reading ﬁle sequential order efﬁcient reading block random order . given signiﬁcant impact seek time disk access , might seem obvious sequential processing faster . however , important understand precisely circumstance sequential ﬁle processing actually faster random access , affect approach designing external sorting algorithm . efﬁcient sequential access relies seek time kept minimum . ﬁrst requirement block making ﬁle fact stored disk sequential order close together , preferably ﬁlling small number contiguous track . least , number extent making ﬁle small . user typically much control layout ﬁle disk , writing ﬁle sequential order disk drive high percentage free space increase likelihood arrangement . second requirement disk drive ’ i/o head remain positioned ﬁle throughout sequential processing . happen competition kind i/o head . example , multiuser timeshared computer sorting process might compete i/o head process user . even sorting process ha sole control i/o head , still likely sequential processing efﬁcient . imagine situation processing done single disk drive , typical arrangement single bank read/write head move together stack platter . sorting process involves reading input ﬁle , alternated writing output ﬁle , i/o head continuously seek input ﬁle output ﬁle . similarly , two input ﬁles processed simultaneously merge process , i/o head continuously seek two ﬁles . sec . 8.5 external sorting 285 moral , single disk drive , often thing efﬁ cient sequential processing data ﬁle . thus , sorting algorithm might efﬁcient performs smaller number nonsequential disk operation rather larger number logically sequential disk operation require large number seek practice . mentioned previously , record size might quite large compared size key.trees typically used ganize large database must support record insertion , deletion , key range search . section 10.2 brieﬂy describes isam , tentative step toward solving problem storing large database must support insertion deletion record . shortcoming help illustrate value tree indexing techniques.274 chap . 8 file processing external sorting example illustrates important keep disk ﬁles coming fragmented , socalled “ disk defragmenters ” speed ﬁle processing time . file fragmentation happens commonly disk nearly full ﬁle manager must search free space whenever ﬁle created changed . 8.3 buﬀers buﬀer pool given speciﬁcations disk drive example 8.1 , ﬁnd take 9.5+11.1×1.5 = 26.2 read one track data average . take 9.5+11.1/2+1/256×11.1 = 15.1 average read single sector data . good saving slightly half time , less 1 data track read . want read single byte , would save u effectively time required read entire sector . reason , nearly disk drive automatically read write entire sector ’ worth information whenever disk accessed , even one byte information requested . sector read , information stored main memory . known buffering caching information . next disk request sector , necessary read disk information already stored main memory . buffering example one method minimizing disk access mentioned beginning chapter : bring additional information disk satisfy future request . information ﬁles accessed random , chance two consecutive disk request sector would low . however , practice disk request close location logical ﬁle least previous request . mean probability next request “ hitting cache ” much higher chance would indicate . principle explains one reason average access time new disk drive lower past . hardware faster , information also stored using better algorithm larger cache minimize number time information need fetched disk . concept also used store part program faster memory within cpu , using cpu cache prevalent modern microprocessor . sectorlevel buffering normally provided operating system ten built directly disk drive controller hardware . operating system maintain least two buffer , one input one output . consider would happen one buffer bytebybyte copy operation . sector containing ﬁrst byte would read i/o buffer . output operation would need destroy content single i/o buffer write byte . buffer would need ﬁlled disk second byte , sec . 8.3 buﬀers buﬀer pool 275 destroyed output . simple solution problem keep one buffer input , second output . disk drive controller operate independently cpu i/o request received . useful cpu typically execute million instruction time required single i/o operation .']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because the output perfectly addresses the input request by identifying the key learning concepts for indexing without any irrelevant content. Great job!, error: None)\n",
      "  - ❌ Contextual Precision (score: 0.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.00 because the first node in the retrieval context is irrelevant, as it 'predominantly discusses concepts related to indexing in databases and does not mention the topics listed in the expected output, such as 'data structure', 'algorithm analysis', or 'recursive algorithms'.', error: None)\n",
      "  - ❌ Contextual Recall (score: 0.21428571428571427, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 0.21 because only a few elements from the expected output, specifically related to 'search' and 'sort', can be traced back to the 1st node in the retrieval context. Most of the expected output sentences do not have corresponding information in the retrieval context., error: None)\n",
      "  - ✅ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4o, reason: The score is 1.00 because there are no contradictions between the actual output and the retrieval context. Great job maintaining consistency!, error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: Identify the 5 most important learning concepts for chapter Indexing. The relevant context can be found here: classic example large database record multiple search key , requiring ability insert , delete , search record . hashing provides outstanding performance situation , limited case search form “ ﬁnd record key value k. ” many application require general search capability . one exam ple range query search record whose key lie within range . query might involve visiting record order key value , ﬁnding record greatest key value . hash table organized support query efﬁciently . chapter introduces ﬁle structure used organize large collection record stored disk . ﬁle structure support efﬁcient insertion , deletion , search operation , exactmatch query , range query , largest/smallest key value search . discussing ﬁle structure , must become familiar ba sic ﬁleprocessing terminology . entrysequenced ﬁle store record order added ﬁle . entrysequenced ﬁles diskbased equivalent unsorted list support efﬁcient search . natural solution sort record order search key . however , typical database , collection employee customer record maintained business , might con tain multiple search key . answer question particular customer might require search name customer . business often wish sort output record zip code order bulk mailing . government paperwork might require ability search social security number . thus , might single “ correct ” order store record . indexing process associating key location correspond ing data record . section 8.5 discussed concept key sort , index ﬁle created whose record consist key/pointer pair . , key asso ciated pointer complete record main database ﬁle . index ﬁle 341 342 chap . 10 indexing could sorted organized using tree structure , thereby imposing logical der record without physically rearranging . one database might several associated index ﬁles , supporting efﬁcient access different key ﬁeld . record database normally ha unique identiﬁer , called primary key . example , primary key set personnel record might social security number id number individual . unfortunately , id number generally inconvenient value perform search searcher unlikely know . instead , searcher might know desired employee ’ name . alternatively , searcher might interested ﬁnding employee whose salary certain range . typical search request database , name salary ﬁelds deserve separate index . however , key value name salary index likely unique . key ﬁeld salary , particular key value might duplicated multiple record , called secondary key . search performed using secondary key . secondary key index simply , secondary index associate secondary key value primary key record secondary key value . point , full database might searched directly record primary key , might primary key index primary index relates primary key value pointer actual record disk . latter case , primary index provides location actual record disk , secondary index refer primary index . indexing important technique organizing large database , many indexing method developed . direct access hashing discussed section 9.4. simple list sorted key value also serve index record ﬁle . indexing disk ﬁles sorted list discussed following section . unfortunately , sorted list doe perform well insert delete operation . third approach indexing tree index.under extreme condition , many search might eventually lead system overﬂow area . “ solution ” problem periodically reorganize entire database . mean rebalancing record 1recall section 8.2.1 cylinder track readable particular placement head multiple platter disk drive . 348 chap . 10 indexing among cylinder , sorting record within cylinder , updating system index table withincylinder block table . reorganization wa typical database system 1960s would normally done night weekly . 10.3 treebased indexing linear indexing efﬁcient database static , , record inserted deleted rarely never . isam adequate limited number update , frequent change . ha essentially two level indexing , isam also break truly large database number cylinder great toplevel index ﬁt main memory . general form , database application following character istics : 1.trees typically used ganize large database must support record insertion , deletion , key range search . section 10.2 brieﬂy describes isam , tentative step toward solving problem storing large database must support insertion deletion record . shortcoming help illustrate value tree indexing techniques.4 ..\n",
      "  - actual output: five important learning concept chapter indexing : 1. indexing fundamental : understanding basic concept indexing , involves associating key location corresponding data record . crucial enabling efficient search operation , especially large database multiple search key involved . 2. primary secondary key : differentiating primary key unique identifier record secondary key nonunique field may used searching . recognizing role primary secondary index facilitating various type query essential effective database management . 3. type index structure : familiarity different indexing method , including sorted list , hash table , treebased structure like btrees . method ha strength weakness , particularly concerning insertion , deletion , search operation , critical optimizing database performance . 4. handling multiple search key : understanding need multiple index support different search key database . includes creation separate index field like name salary , may unique commonly used search query . 5. reorganization maintenance index : recognizing importance periodically reorganizing maintaining index ensure efficient performance , especially dynamic database record frequently inserted deleted . includes understanding technique like rebalancing implication database growth indexing strategy . concept provide foundational understanding indexing work database various strategy employed optimize data retrieval management .\n",
      "  - expected output: data structure basic algorithm analysis notation algorithm nonrecursive algorthims nonrecursive algorithm search linear search binary search sort insertion sort selection sort bubble sort recursive algorithm recursive binary search recursive sort merge sort quick sort\n",
      "  - context: None\n",
      "  - retrieval context: ['classic example large database record multiple search key , requiring ability insert , delete , search record . hashing provides outstanding performance situation , limited case search form “ ﬁnd record key value k. ” many application require general search capability . one exam ple range query search record whose key lie within range . query might involve visiting record order key value , ﬁnding record greatest key value . hash table organized support query efﬁciently . chapter introduces ﬁle structure used organize large collection record stored disk . ﬁle structure support efﬁcient insertion , deletion , search operation , exactmatch query , range query , largest/smallest key value search . discussing ﬁle structure , must become familiar ba sic ﬁleprocessing terminology . entrysequenced ﬁle store record order added ﬁle . entrysequenced ﬁles diskbased equivalent unsorted list support efﬁcient search . natural solution sort record order search key . however , typical database , collection employee customer record maintained business , might con tain multiple search key . answer question particular customer might require search name customer . business often wish sort output record zip code order bulk mailing . government paperwork might require ability search social security number . thus , might single “ correct ” order store record . indexing process associating key location correspond ing data record . section 8.5 discussed concept key sort , index ﬁle created whose record consist key/pointer pair . , key asso ciated pointer complete record main database ﬁle . index ﬁle 341 342 chap . 10 indexing could sorted organized using tree structure , thereby imposing logical der record without physically rearranging . one database might several associated index ﬁles , supporting efﬁcient access different key ﬁeld . record database normally ha unique identiﬁer , called primary key . example , primary key set personnel record might social security number id number individual . unfortunately , id number generally inconvenient value perform search searcher unlikely know . instead , searcher might know desired employee ’ name . alternatively , searcher might interested ﬁnding employee whose salary certain range . typical search request database , name salary ﬁelds deserve separate index . however , key value name salary index likely unique . key ﬁeld salary , particular key value might duplicated multiple record , called secondary key . search performed using secondary key . secondary key index simply , secondary index associate secondary key value primary key record secondary key value . point , full database might searched directly record primary key , might primary key index primary index relates primary key value pointer actual record disk . latter case , primary index provides location actual record disk , secondary index refer primary index . indexing important technique organizing large database , many indexing method developed . direct access hashing discussed section 9.4. simple list sorted key value also serve index record ﬁle . indexing disk ﬁles sorted list discussed following section . unfortunately , sorted list doe perform well insert delete operation . third approach indexing tree index.under extreme condition , many search might eventually lead system overﬂow area . “ solution ” problem periodically reorganize entire database . mean rebalancing record 1recall section 8.2.1 cylinder track readable particular placement head multiple platter disk drive . 348 chap . 10 indexing among cylinder , sorting record within cylinder , updating system index table withincylinder block table . reorganization wa typical database system 1960s would normally done night weekly . 10.3 treebased indexing linear indexing efﬁcient database static , , record inserted deleted rarely never . isam adequate limited number update , frequent change . ha essentially two level indexing , isam also break truly large database number cylinder great toplevel index ﬁt main memory . general form , database application following character istics : 1.trees typically used ganize large database must support record insertion , deletion , key range search . section 10.2 brieﬂy describes isam , tentative step toward solving problem storing large database must support insertion deletion record . shortcoming help illustrate value tree indexing techniques.4 .']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "Contextual Precision: 75.00% pass rate\n",
      "Contextual Recall: 25.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">✓</span> Tests finished 🎉! Run <span style=\"color: #008000; text-decoration-color: #008000\">'deepeval login'</span> to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141m✓\u001b[0m Tests finished 🎉! Run \u001b[32m'deepeval login'\u001b[0m to save and analyze evaluation results on Confident AI. \n",
       "‼️  Friendly reminder 😇: You can also run evaluations with ALL of deepeval's metrics directly on Confident AI \n",
       "instead.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.utils import normalize_text\n",
    "\n",
    "normalized_concepts = [[normalize_text(' '.join(t))] for t in concepts]\n",
    "normalized_truths = [normalize_text(t) for t in actual_concepts]\n",
    "\n",
    "normalized_retrieved = {}\n",
    "for k in retrieved.keys():\n",
    "    normalized_retrieved[k] = normalize_text(retrieved[k])\n",
    "\n",
    "# for i in range(5):\n",
    "normalized_samples = extractor.evaluate('concepts', 5, normalized_concepts, normalized_truths, data = normalized_retrieved, metrics = metrics)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "56211c6593777bedeb9a19153ebc7701344247d055e998aec252a1f471490a08"
  },
  "kernelspec": {
   "display_name": "Python 3.10.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
